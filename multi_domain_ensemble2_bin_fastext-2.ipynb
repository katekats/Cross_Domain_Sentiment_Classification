{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#import json\n",
    "#import optparse\n",
    "import os, regex as re\n",
    "import pandas as pd\n",
    "\n",
    "#import libraries \n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re \n",
    "#import matplotlib.pyplot as plt \n",
    "#import seaborn as sns\n",
    "import scipy as sp\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open file 'Downloads/processed_acl/books/negative.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/books/positive.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/dvd/negative.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/dvd/positive.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/kitchen/negative.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/kitchen/positive.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/electronics/negative.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/electronics/positive.review', mode 'r+' at 0x11456e420>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "STOPWORDS = set(('i','im','ive', 'me','my','myself','we','our','ours','ourselves','you','youre','youve','youll','youd','your','yours','yourself','yourselves','he','him','his','himself','she','shes','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','thatll','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','only','own','same','so','than','too','very','s','t','can','will','just','should','shouldve','now','d','ll','m','o','re','ve','y','ma'))\n",
    "\n",
    "SNO = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def read_data(filename):\n",
    "    reviews = []\n",
    "    with open(filename, 'r') as fr:\n",
    "        end_of_review = False\n",
    "        for line in fr:\n",
    "            line = re.sub(r'[:][\\d]', \" \", line)\n",
    "            if \"#label#:negative\" in line:\n",
    "                line = line.replace(\"#label#:negative\", \"\")\n",
    "                end_of_review = True\n",
    "            elif \"#label#:positive\" in line:\n",
    "                line = line.replace(\"#label#:positive\", \"\")\n",
    "                end_of_review = True\n",
    "            \n",
    "            if end_of_review:\n",
    "                reviews.append(line.strip())\n",
    "                end_of_review = False           \n",
    "    return reviews\n",
    "\n",
    "def process_data(path, sentiment):\n",
    "    reviews_list = read_data(path + sentiment + '.review')\n",
    "    df = pd.DataFrame({'reviews': reviews_list})\n",
    "    df['label'] = 1 if sentiment == \"positive\" else 0\n",
    "    df['#words'] = df['reviews'].apply(lambda x: len(x.split()))\n",
    "    df['code'] = path.split(\"/\")[-2]\n",
    "    return df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def main():\n",
    "    base_path = 'Downloads/processed_acl/'\n",
    "    categories = ['books', 'dvd', 'kitchen', 'electronics']\n",
    "    df_list = []\n",
    "    \n",
    "    for category in categories:\n",
    "        for sentiment in ['positive', 'negative']:\n",
    "            df_list.append(process_data(base_path + category + '/', sentiment))\n",
    "\n",
    "    # Example concatenation:\n",
    "    df_books_dvd = pd.concat([df_list[0], df_list[2]], ignore_index=True)\n",
    "    # You can continue with other combinations as needed...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_electronics\n",
    "#Functions for preprocessing steps\n",
    "stop = set(('i','im','ive', 'me','my','myself','we','our','ours','ourselves','you','youre','youve','youll','youd','your','yours','yourself','yourselves','he','him','his','himself','she','shes','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','thatll','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','only','own','same','so','than','too','very','s','t','can','will','just','should','shouldve','now','d','ll','m','o','re','ve','y','ma'))\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def replace_url(df,col,rm1,rm2):\n",
    "    return(df[col].str.replace(rm1,rm2))\n",
    "\n",
    "def extract_emo(df, col, emo):\n",
    "    return(df[col].str.extractall(emo).unstack().apply(lambda x:' '.join(x.dropna()), axis=1))\n",
    "\n",
    "def replace_emo(df,col,emo1,emo2):\n",
    "    return(df[col].str.replace(emo1,emo2))\n",
    "\n",
    "def replace_punct(df, col, punct1, punct2):\n",
    "    return(df[col].str.replace(punct1, punct2))\n",
    "\n",
    "def remove_numbers(df,col,rm1,rm2):\n",
    "    return(df[col].str.replace(rm1,rm2))\n",
    "\n",
    "def lower_words(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join(x.lower() for x in x.split())))\n",
    "\n",
    "def remove_stop(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)))\n",
    "\n",
    "def tokenize(df,col):\n",
    "    return(df.apply(lambda row: nltk.word_tokenize(row[col]), axis=1))\n",
    "\n",
    "def word_count(df,col):\n",
    "    return(df[col].apply(lambda x: len(str(x).split(' '))))\n",
    "\n",
    "def stemming(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join([sno.stem(word) for word in x.split()])))\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df['nohtml'] = replace_url(df,'reviews','^http?:\\/\\/.*[\\r\\n]*','')\n",
    "    df['nohtml'] = lower_words(df,'nohtml')\n",
    "    df['nohtml'] = remove_numbers(df, 'nohtml', '[0-9]+',' ')\n",
    "    df['nohtml'] = replace_punct(df, 'nohtml', '[^\\w\\s]',' ')\n",
    "    df['nohtml'] = replace_punct(df, 'nohtml', '_',' ')\n",
    "    df['nohtml'] = replace_punct(df, 'nohtml',r'\\b(no|not|nt|dont|doesnt|doesn|don|didnt|cant|cannt|cannot|wouldnt|wont|couldnt|hasnt|havent|hadnt|shouldnt)\\s+([a-z])',r'not \\2')\n",
    "    df['nohtml'] = remove_stop(df,'nohtml')\n",
    "    df['tokenized'] = tokenize(df,'nohtml')\n",
    "    df['#token'] = word_count(df,'tokenized')\n",
    "    return df\n",
    "\n",
    "# Preprocess the sample dataset\n",
    "sample_df = preprocess_data(df_electronics)\n",
    "\n",
    "# Split based on word count\n",
    "tokens_thresholds = {\n",
    "    'books': 80,\n",
    "    'dvd': 75,\n",
    "    'electronics': 55,\n",
    "    'kitchen': 54\n",
    "}\n",
    "\n",
    "sub_datasets = {}\n",
    "for category, threshold in tokens_thresholds.items():\n",
    "    sub_datasets[category] = sample_df[sample_df[\"#token\"] > threshold].reset_index(drop=True)\n",
    "\n",
    "# Combine datasets\n",
    "combinations = [\n",
    "    ('books', 'dvd'),\n",
    "    ('books', 'kitchen'),\n",
    "    ('dvd', 'books'),\n",
    "    ('electronics', 'books'),\n",
    "    ('kitchen', 'books'),\n",
    "    ('electronics', 'dvd'),\n",
    "    ('kitchen', 'dvd'),\n",
    "    ('books', 'electronics'),\n",
    "    ('dvd', 'electronics'),\n",
    "    ('kitchen', 'electronics'),\n",
    "    ('electronics', 'kitchen'),\n",
    "    ('dvd', 'kitchen')\n",
    "]\n",
    "\n",
    "combined_datasets = {}\n",
    "for combo in combinations:\n",
    "    key = f\"{combo[0]}_{combo[1]}\"\n",
    "    combined_datasets[key] = sub_datasets[combo[0]].append(sub_datasets[combo[1]], ignore_index=True)\n",
    "\n",
    "# If you want to get one of the combined datasets:\n",
    "sample_df1 = combined_datasets['electronics_kitchen'].copy(deep=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikaterinikatsarou/homebrew_high_sierra/lib/python2.7/site-packages/ipykernel_launcher.py:56: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# chi square for the important features per product category\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk.tag import pos_tag\n",
    "from fastText import load_model\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def extract_important_features(df, text_column, target_column, n_features=1000):\n",
    "    count_vect = CountVectorizer()\n",
    "    X = count_vect.fit_transform(df[text_column].values)\n",
    "    feature_names = count_vect.get_feature_namessample_df = sample_df1.copy()()   \n",
    "    chi2_scores = chi2(X, df[target_column])\n",
    "    sorted_scores = sorted(zip(feature_names, chi2_scores[0]), key=lambda x:x[1])[-n_features:]\n",
    "    return [k for k, v in sorted_scores if v < 0.05]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikaterinikatsarou/homebrew_high_sierra/lib/python2.7/site-packages/ipykernel_launcher.py:6: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def exclude_nouns(token_list):\n",
    "    return [word for word, pos in pos_tag(token_list) if not pos.startswith('N')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import FastText bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_fasttext_data(df, token_column, model):\n",
    "    all_words = set(w for tokens in df[token_column] for w in tokens)\n",
    "    return {word: model.get_word_vector(word).astype('float32') for word in all_words}\n",
    "\n",
    "# Load fastText model (uncomment the appropriate line)\n",
    "# ft_model = FastText(\"Downloads/cc.en.300.bin\")\n",
    "# ft_model = fastText.load_model('Downloads/cc.en.300.bin')\n",
    "ft_model = load_model('Downloads/cc.en.300.bin')\n",
    "\n",
    "# Extract important features based on Chi-squared scores\n",
    "cat_features = extract_important_features(sample_df, 'nohtml', 'code')\n",
    "label_features = extract_important_features(sample_df, 'nohtml', 'label')\n",
    "\n",
    "# Exclude nouns and create a new column 'tokenized2'\n",
    "sample_df[\"tokenized2\"] = sample_df[\"tokenized\"].apply(exclude_nouns)\n",
    "\n",
    "# Extract FastText vectors\n",
    "fasttext_dict = df_to_fasttext_data(sample_df, 'tokenized', ft_model)\n",
    "fasttext2_dict = df_to_fasttext_data(sample_df, 'tokenized2', ft_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = df_to_data(sample_df, X)\n",
    "fasttext2 = df_to_data(sample_df, X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the pretrained fasttext\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "#from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class BaseEmbeddingVectorizer:\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec[next(iter(word2vec))]) if word2vec else None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'word2vec': self.word2vec}\n",
    "\n",
    "\n",
    "class MeanEmbeddingVectorizer(BaseEmbeddingVectorizer):\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "class TfidfEmbeddingVectorizer(BaseEmbeddingVectorizer):\n",
    "    def fit(self, X, y=None):\n",
    "        self.tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        self.tfidf.fit(X)\n",
    "        max_idf = max(self.tfidf.idf_)\n",
    "        self.word2weight = defaultdict(lambda: max_idf,\n",
    "                                       [(w, self.tfidf.idf_[i]) for w, i in self.tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] * self.word2weight[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "# If you need the version with \"fasttext2\", just instantiate the above classes with different embeddings:\n",
    "# mean_vectorizer = MeanEmbeddingVectorizer(fasttext2)\n",
    "# tfidf_vectorizer = TfidfEmbeddingVectorizer(fasttext2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from mlxtend.preprocessing import DenseTransformer \n",
    "    \n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ColumnSelector(BaseEstimator):\n",
    "    \"\"\"Object for selecting specific columns from a data set.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols : array-like (default: None)\n",
    "        A list specifying the feature indices to be selected. For example,\n",
    "        [1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and\n",
    "        ['A','C','D'] to select the name of feature columns A, C and D.\n",
    "        If None, returns all columns in the array.\n",
    "    drop_axis : bool (default=False)\n",
    "        Drops last axis if True and the only one column is selected. This\n",
    "        is useful, e.g., when the ColumnSelector is used for selecting\n",
    "        only one column and the resulting array should be fed to e.g.,\n",
    "        a scikit-learn column selector. E.g., instead of returning an\n",
    "        array with shape (n_samples, 1), drop_axis=True will return an\n",
    "        aray with shape (n_samples,).\n",
    "    Examples\n",
    "    -----------\n",
    "    For usage examples, please see\n",
    "    http://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cols=None, drop_axis=False):\n",
    "        self.cols = cols\n",
    "        self.drop_axis = drop_axis\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "    \"\"\" Return a slice of the input array. ... [rest of the docstring] ... \"\"\"\n",
    "\n",
    "    # We use the loc or iloc accessor if the input is a pandas dataframe\n",
    "    if hasattr(X, 'loc') or hasattr(X, 'iloc'):\n",
    "        # Ensure cols is a list\n",
    "        if isinstance(self.cols, tuple):\n",
    "            self.cols = list(self.cols)\n",
    "        \n",
    "        # Check all elements in `cols` are of the same data type\n",
    "        types = {type(i) for i in self.cols}\n",
    "        if len(types) > 1:\n",
    "            raise ValueError('Elements in `cols` should be all of the same data type.')\n",
    "        \n",
    "        if isinstance(self.cols[0], int):\n",
    "            t = X.iloc[:, self.cols].values\n",
    "        elif isinstance(self.cols[0], str):\n",
    "            t = X.loc[:, self.cols].values\n",
    "        else:\n",
    "            raise ValueError('Elements in `cols` should be either `int` or `str`.')\n",
    "    else:\n",
    "        t = X[:, self.cols]\n",
    "\n",
    "    if t.shape[-1] == 1 and self.drop_axis:\n",
    "        t = t.reshape(-1)\n",
    "    elif len(t.shape) == 1 and not self.drop_axis:\n",
    "        t = t[:, np.newaxis]\n",
    "    \n",
    "    return t\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_pipeline(column, vectorizer, classifier):\n",
    "    return Pipeline([\n",
    "        (\"col_sel\", ColumnSelector(cols=column, drop_axis=True)),\n",
    "        (\"vectorizer\", vectorizer),\n",
    "        (\"classifier\", classifier)\n",
    "    ])\n",
    "\n",
    "# Define classifiers\n",
    "logistic_l2 = LogisticRegression(\"l2\", random_state=0)\n",
    "svc_linear = SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True)\n",
    "\n",
    "# Construct pipelines\n",
    "log_reg_fasttext_tfidf = create_pipeline(5, TfidfEmbeddingVectorizer(fasttext), logistic_l2)\n",
    "log_reg_fasttext2 = create_pipeline(5, MeanEmbeddingVectorizer(fasttext), logistic_l2)\n",
    "svm_fasttext = create_pipeline(5, MeanEmbeddingVectorizer(fasttext), svc_linear)\n",
    "log_reg_fasttext_tfidf2 = create_pipeline(7, TfidfEmbeddingVectorizer(fasttext2), logistic_l2)\n",
    "svm_fasttext_tfidf = create_pipeline(5, TfidfEmbeddingVectorizer(fasttext), svc_linear)\n",
    "\n",
    "# Add any more pipelines as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikaterinikatsarou/homebrew_high_sierra/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7555012224938875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mlxtend.classifier import SuperLearner\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "TRAIN_SIZE = 0.9  # 90% training, adjust as needed\n",
    "COL_INDEX = 7     # adjust as per your specific requirements\n",
    "\n",
    "def build_classifier_pipelines():\n",
    "    \"\"\"Create and return classifier pipelines.\"\"\"\n",
    "    \n",
    "    log_reg_fasttext_tfidf = Pipeline([\n",
    "        (\"col_sel\", ColumnSelector(cols=COL_INDEX, drop_axis=True)),\n",
    "        (\"fasttext vectorizer\", TfidfEmbeddingVectorizer(fasttext)),\n",
    "        (\"log_reg\", LogisticRegression(\"l2\", random_state=RANDOM_STATE))\n",
    "    ])\n",
    "    \n",
    "    svm_fasttext = Pipeline([\n",
    "        (\"col_sel\", ColumnSelector(cols=COL_INDEX, drop_axis=True)),\n",
    "        (\"fasttext vectorizer\", MeanEmbeddingVectorizer(fasttext)),\n",
    "        (\"LinearSVC\", SVC(random_state=RANDOM_STATE, kernel=\"linear\", tol=1e-5, probability=True))\n",
    "    ])\n",
    "    \n",
    "    # ... [other pipelines]\n",
    "    \n",
    "    return [svm_fasttext, svm_fasttext_tfidf, log_reg_fasttext_tfidf2]\n",
    "\n",
    "def train_ensemble(X_train, y_train):\n",
    "    \"\"\"Train ensemble using given training data.\"\"\"\n",
    "    \n",
    "    ensemble = SuperLearner(scorer=accuracy_score, random_state=RANDOM_STATE)\n",
    "    ensemble.add(build_classifier_pipelines())\n",
    "    ensemble.add_meta(LogisticRegression(\"l2\", random_state=RANDOM_STATE))\n",
    "    \n",
    "    ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "def main():\n",
    "    # Assuming `sample_df` and `sample_df.label` are your features and labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        sample_df.values, sample_df.label.values, \n",
    "        train_size=TRAIN_SIZE, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    ensemble_model = train_ensemble(X_train, y_train)\n",
    "    preds = ensemble_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c589eab13420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_df' is not defined"
     ]
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
