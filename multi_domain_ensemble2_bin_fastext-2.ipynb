{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#import json\n",
    "#import optparse\n",
    "import os, regex as re\n",
    "import pandas as pd\n",
    "\n",
    "#import libraries \n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re \n",
    "#import matplotlib.pyplot as plt \n",
    "#import seaborn as sns\n",
    "import scipy as sp\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open file 'Downloads/processed_acl/books/negative.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/books/positive.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/dvd/negative.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/dvd/positive.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/kitchen/negative.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/kitchen/positive.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/electronics/negative.review', mode 'r+' at 0x11456e420>\n",
      "<open file 'Downloads/processed_acl/electronics/positive.review', mode 'r+' at 0x11456e420>\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    list1 = []\n",
    "    reviews = []\n",
    "    with open(filename, 'r+') as fr:\n",
    "        print(fr)\n",
    "        end_of_review = 0\n",
    "        for line in fr:\n",
    "            #print(\"is not empty\")\n",
    "            line = re.sub(r'[:][\\d]', \" \", str(line))\n",
    "            if (re.search(\"#label#:negative\", str(line))):\n",
    "                line = re.sub(\"#label#:negative\", \" \" ,str(line))\n",
    "                end_of_review=1\n",
    "            if (re.search(\"#label#:positive\", str(line))):\n",
    "                line = re.sub(\"#label#:positive\", \" \" ,str(line))\n",
    "                end_of_review=1    \n",
    "            str1 = str(line)    \n",
    "                #print(\"end-of review\")\n",
    "            if end_of_review == 1:    \n",
    "                reviews.append(str1)\n",
    "                end_of_review = 0           \n",
    "           \n",
    "        return (reviews)\n",
    "            #print(list1)      \n",
    "        \n",
    "def convert_to_dataframe(listname):\n",
    "    df1 = pd.DataFrame({'reviews':listname})\n",
    "    return df1\n",
    "\n",
    "def get_label_from_filename(filename, df):\n",
    "    if re.search(\"positive\", str(filename)):\n",
    "        df[\"label\"] = 1\n",
    "    if  re.search(\"negative\", str(filename)):\n",
    "        df[\"label\"] = 0 \n",
    "        #pd.set_option('display.max_colwidth', -1)\n",
    "    return df   \n",
    "\n",
    "# books_dataset\n",
    "neg_reviews_list = read_data('Downloads/processed_acl/books/negative.review')\n",
    "df1 = convert_to_dataframe(neg_reviews_list)\n",
    "df1 =get_label_from_filename('Downloads/processed_acl/books/negative.review', df1)\n",
    "\n",
    "\n",
    "pos_reviews_list = read_data('Downloads/processed_acl/books/positive.review')\n",
    "df2 = convert_to_dataframe(pos_reviews_list)\n",
    "df2 =get_label_from_filename('Downloads/processed_acl/books/positive.review', df2)\n",
    "\n",
    "\n",
    "df_books = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# dvd_dataset\n",
    "neg_reviews_list2 = read_data('Downloads/processed_acl/dvd/negative.review')\n",
    "df3 = convert_to_dataframe(neg_reviews_list2)\n",
    "df3 =get_label_from_filename('Downloads/processed_acl/dvd/negative.review', df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pos_reviews_list2 = read_data('Downloads/processed_acl/dvd/positive.review')\n",
    "df4 = convert_to_dataframe(pos_reviews_list2)\n",
    "df4 =get_label_from_filename('Downloads/processed_acl/dvd/positive.review', df4)\n",
    "\n",
    "df_dvd = pd.concat([df3, df4], axis=0)\n",
    "\n",
    "# kitchen_dataset\n",
    "neg_reviews_list3 = read_data('Downloads/processed_acl/kitchen/negative.review')\n",
    "df5 = convert_to_dataframe(neg_reviews_list3)\n",
    "df5 =get_label_from_filename('Downloads/processed_acl/kitchen/negative.review', df5)\n",
    "\n",
    "\n",
    "pos_reviews_list3 = read_data('Downloads/processed_acl/kitchen/positive.review')\n",
    "df6 = convert_to_dataframe(pos_reviews_list3)\n",
    "df6 =get_label_from_filename('Downloads/processed_acl/kitchen/positive.review', df6)\n",
    "\n",
    "df_kitchen = pd.concat([df5, df6], axis=0)\n",
    "\n",
    "# electronics_dataset\n",
    "neg_reviews_list4 = read_data('Downloads/processed_acl/electronics/negative.review')\n",
    "df7 = convert_to_dataframe(neg_reviews_list4)\n",
    "df7 =get_label_from_filename('Downloads/processed_acl/electronics/negative.review', df7)\n",
    "\n",
    "\n",
    "pos_reviews_list4 = read_data('Downloads/processed_acl/electronics/positive.review')\n",
    "df8 = convert_to_dataframe(pos_reviews_list4)\n",
    "df8 =get_label_from_filename('Downloads/processed_acl/electronics/positive.review', df8)\n",
    "\n",
    "df_electronics = pd.concat([df7, df8], axis=0)\n",
    "\n",
    "\n",
    "#adding column for number of words in review in original data frame\n",
    "df_books['#words'] = df_books.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "df_dvd['#words'] = df_dvd.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "#e_df['#words'] = e_df.reviewText.apply(lambda x: len(str(x).split(' ')))\n",
    "#k_df['#words'] = k_df.reviewText.apply(lambda x: len(str(x).split(' ')))\n",
    "df_kitchen['#words'] = df_kitchen.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "df_electronics['#words'] = df_electronics.apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "#Shuffling the rows in all the datasets to make them randomly ordered\n",
    "df_books.sample(frac=1)\n",
    "df_books = df_books.sample(frac=1).reset_index(drop=True)\n",
    "df_books[\"code\"] = \"books\"\n",
    "\n",
    "df_dvd.sample(frac=1)\n",
    "df_dvd = df_dvd.sample(frac=1).reset_index(drop=True)\n",
    "df_dvd[\"code\"] = \"dvd\"\n",
    "\n",
    "df_kitchen.sample(frac=1)\n",
    "df_kitchen = df_kitchen.sample(frac=1).reset_index(drop=True)\n",
    "df_kitchen[\"code\"] = \"kitchen\"\n",
    "\n",
    "df_electronics.sample(frac=1)\n",
    "df_electronics = df_electronics.sample(frac=1).reset_index(drop=True)\n",
    "df_electronics[\"code\"] = \"electronics\"\n",
    "\n",
    "#Appending the datasets CDSA \n",
    "bd = df_books.append(df_dvd, ignore_index=True)\n",
    "bk = df_books.append(df_kitchen, ignore_index=True)\n",
    "db = df_dvd.append(df_books, ignore_index=True)\n",
    "eb = df_electronics.append(df_books, ignore_index=True)\n",
    "kb = df_kitchen.append(df_books, ignore_index=True)\n",
    "ed = df_electronics.append(df_dvd, ignore_index=True)\n",
    "kd = df_kitchen.append(df_dvd, ignore_index=True)\n",
    "be = df_books.append(df_electronics, ignore_index=True)\n",
    "de = df_dvd.append(df_electronics, ignore_index=True)\n",
    "ke = df_kitchen.append(df_electronics, ignore_index=True)\n",
    "ek = df_electronics.append(df_kitchen, ignore_index=True)\n",
    "dk = df_dvd.append(df_kitchen, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_electronics\n",
    "#Functions for preprocessing steps\n",
    "stop = set(('i','im','ive', 'me','my','myself','we','our','ours','ourselves','you','youre','youve','youll','youd','your','yours','yourself','yourselves','he','him','his','himself','she','shes','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','thatll','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','only','own','same','so','than','too','very','s','t','can','will','just','should','shouldve','now','d','ll','m','o','re','ve','y','ma'))\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def replace_url(df,col,rm1,rm2):\n",
    "    return(df[col].str.replace(rm1,rm2))\n",
    "\n",
    "def extract_emo(df, col, emo):\n",
    "    return(df[col].str.extractall(emo).unstack().apply(lambda x:' '.join(x.dropna()), axis=1))\n",
    "\n",
    "def replace_emo(df,col,emo1,emo2):\n",
    "    return(df[col].str.replace(emo1,emo2))\n",
    "\n",
    "def replace_punct(df, col, punct1, punct2):\n",
    "    return(df[col].str.replace(punct1, punct2))\n",
    "\n",
    "def remove_numbers(df,col,rm1,rm2):\n",
    "    return(df[col].str.replace(rm1,rm2))\n",
    "\n",
    "def lower_words(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join(x.lower() for x in x.split())))\n",
    "\n",
    "def remove_stop(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)))\n",
    "\n",
    "def tokenize(df,col):\n",
    "    return(df.apply(lambda row: nltk.word_tokenize(row[col]), axis=1))\n",
    "\n",
    "def word_count(df,col):\n",
    "    return(df[col].apply(lambda x: len(str(x).split(' '))))\n",
    "\n",
    "def stemming(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join([sno.stem(word) for word in x.split()])))\n",
    "\n",
    "\n",
    "#Step1 Pre-Processing\n",
    "sample_df['nohtml'] = replace_url(sample_df,'reviews','^http?:\\/\\/.*[\\r\\n]*','')\n",
    "sample_df['nohtml'] = lower_words(sample_df,'nohtml')\n",
    "sample_df['nohtml'] = remove_numbers(sample_df, 'nohtml', '[0-9]+',' ')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml', '[^\\w\\s]',' ')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml', '_',' ')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml',r'\\b(no|not|nt|dont|doesnt|doesn|don|didnt|cant|cannt|cannot|wouldnt|wont|couldnt|hasnt|havent|hadnt|shouldnt)\\s+([a-z])',r'not \\2')\n",
    "sample_df['nohtml'] = remove_stop(sample_df,'nohtml')\n",
    "#sample_df['nohtml'] = stemming(sample_df,'nohtml')\n",
    "sample_df['tokenized'] = tokenize(sample_df,'nohtml')\n",
    "sample_df['#token'] = word_count(sample_df,'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_dvd =sample_df[sample_df[\"#token\"]>75].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_books=sample_df[sample_df[\"#token\"]>80].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_electronics=sample_df[sample_df[\"#token\"]>=55].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_kitchen=sample_df[sample_df[\"#token\"]>=54].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending the datasets CDSA \n",
    "bd = sample_df_books.append(sample_df_dvd, ignore_index=True)\n",
    "bk = sample_df_books.append(sample_df_kitchen, ignore_index=True)\n",
    "db = sample_df_dvd.append(sample_df_books, ignore_index=True)\n",
    "eb = sample_df_electronics.append(sample_df_books, ignore_index=True)\n",
    "kb = sample_df_kitchen.append(sample_df_books, ignore_index=True)\n",
    "ed = sample_df_electronics.append(sample_df_dvd, ignore_index=True)\n",
    "kd = sample_df_kitchen.append(sample_df_dvd, ignore_index=True)\n",
    "be = sample_df_books.append(sample_df_electronics, ignore_index=True)\n",
    "de = sample_df_dvd.append(sample_df_electronics, ignore_index=True)\n",
    "ke = sample_df_kitchen.append(sample_df_electronics, ignore_index=True)\n",
    "ek = sample_df_electronics.append(sample_df_kitchen, ignore_index=True)\n",
    "dk = sample_df_dvd.append(sample_df_kitchen, ignore_index=True)\n",
    "sample_df1=ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikaterinikatsarou/homebrew_high_sierra/lib/python2.7/site-packages/ipykernel_launcher.py:56: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "sample_df = sample_df1.copy()\n",
    "\n",
    "# chi square for the important features per product category\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train1 = count_vect.fit_transform(sample_df.nohtml.values)\n",
    "features1 = count_vect.get_feature_names()   \n",
    "    \n",
    "cat_chi2score0 = chi2(X_train1, sample_df.code)[0]\n",
    "cat_chi2score1 = chi2(X_train1, sample_df.code)[1]\n",
    "cat_wscores = zip(features1, cat_chi2score0)\n",
    "cat_wchi2 = sorted(cat_wscores, key=lambda x:x[1])\n",
    "#topchi2 = list(zip(*wchi2[-1000:]))\n",
    "cat_topchi2score= cat_wchi2[-1000:]\n",
    "#cat_chi2score0\n",
    "\n",
    "\n",
    "#chi square for the important features per sentiment class\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(sample_df.nohtml[0:1638].values)\n",
    "features = count_vect.get_feature_names()   \n",
    "    \n",
    "chi2score0 = chi2(X_train, sample_df.label[0:1638])[0]\n",
    "chi2score1 = chi2(X_train, sample_df.label[0:1638])[1]\n",
    "wscores = zip(features, chi2score0)\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1])\n",
    "#topchi2 = list(zip(*wchi2[-1000:]))\n",
    "\n",
    "topchi2score= wchi2[-6000:]\n",
    "#topchi2score\n",
    "\n",
    "\n",
    "# use only the important features\n",
    "import collections\n",
    "\n",
    "d4 = collections.OrderedDict((k, v) for k, v in zip(features1, cat_chi2score1) if v<0.05)\n",
    "#print(d4)\n",
    "list4 = [k for k, v in d4.items()]\n",
    "d5 = collections.OrderedDict((k, v) for k, v in zip(features, chi2score1) if v<0.05)\n",
    "list5 = [k for k, v in d5.items() if k not in d4.items()]\n",
    "#d2 = collections.OrderedDict((k, v) for k, v in cat_chi2score)\n",
    "d2 = collections.OrderedDict((k, v) for k, v in cat_topchi2score)\n",
    "list3 = [k for k, v in d2.items()]\n",
    "\n",
    "d = collections.OrderedDict((k, v) for k, v in topchi2score)\n",
    "list1 = [k for k, v in d.items() if k not in d2.items()]\n",
    "   \n",
    "# keep the important features    \n",
    "sample_df[\"tokenized\"] = sample_df.tokenized   \n",
    "for  index, row in sample_df[0:1638].iterrows():\n",
    "   \n",
    "    row[\"tokenized\"] =  [word for word in row[\"tokenized\"] if word in list5]\n",
    "    sample_df[0:1638].set_value(index,'tokenized',row[\"tokenized\"])   \n",
    "    \n",
    "    \n",
    "sample_df[\"tokenized2\"] = sample_df.tokenized      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikaterinikatsarou/homebrew_high_sierra/lib/python2.7/site-packages/ipykernel_launcher.py:6: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "noun = []\n",
    "for  index, row in sample_df.iterrows():\n",
    "    noun = [word for word,pos in pos_tag(row[\"tokenized2\"]) if pos.startswith('N')]\n",
    "    #print(noun)\n",
    "    row[\"tokenized2\"] =  [word for word in row[\"tokenized2\"] if word not in noun]\n",
    "    sample_df.set_value(index,'tokenized2',row[\"tokenized2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import FastText bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyfasttext import FastText\n",
    "#ft_model = FastText(\"Downloads/cc.en.300.bin\")\n",
    "\n",
    "X1= sample_df.tokenized2\n",
    "X=sample_df.tokenized\n",
    "\n",
    "\n",
    "from fastText import load_model\n",
    "\n",
    "ft_model = load_model('Downloads/cc.en.300.bin')\n",
    "n_features = ft_model.get_dimension()\n",
    "dict1 ={}\n",
    "\n",
    "\n",
    "def df_to_data(df, X):\n",
    "    \"\"\"\n",
    "    Convert a given dataframe to a dataset of inputs for the NN.\n",
    "    \"\"\"\n",
    "    #x = np.zeros((len(df), 1000, n_features), dtype='float32')\n",
    "\n",
    "    #for i, word in enumerate(sample_df['tokenized'].values):\n",
    "    X=sample_df.tokenized\n",
    "    all_words = set(w for words in X for w in words)\n",
    "    for word in all_words:\n",
    "            nums=ft_model.get_word_vector(word).astype('float32')\n",
    "            dict1[word] = nums\n",
    "            \n",
    "     \n",
    "    return dict1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = df_to_data(sample_df, X)\n",
    "fasttext2 = df_to_data(sample_df, X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the pretrained fasttext\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "#from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y= sample_df.label\n",
    "import struct \n",
    "\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, probs=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.probs = probs\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(fasttext))])\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(word2vec=self.word2vec)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "   \n",
    "    \n",
    "  \n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, probs=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.probs = probs\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(fasttext))])\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(word2vec=self.word2vec)        \n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "class MeanEmbeddingVectorizer2(object):\n",
    "    def __init__(self, word2vec, probs=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.probs = probs\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(fasttext2))])\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(word2vec=self.word2vec)\n",
    "    \n",
    "    def fit(self, X1, y):\n",
    "        return self\n",
    "            \n",
    " \n",
    "\n",
    "    def transform(self, X1):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X1\n",
    "        ])\n",
    "    \n",
    "    \n",
    "     \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer2(object):\n",
    "    def __init__(self, word2vec, probs=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.probs = probs\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(fasttext2))])\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(word2vec=self.word2vec)        \n",
    "        \n",
    "    def fit(self, X1,y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X1)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X1):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X1\n",
    "            ])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from mlxtend.preprocessing import DenseTransformer \n",
    "    \n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ColumnSelector(BaseEstimator):\n",
    "    \"\"\"Object for selecting specific columns from a data set.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols : array-like (default: None)\n",
    "        A list specifying the feature indices to be selected. For example,\n",
    "        [1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and\n",
    "        ['A','C','D'] to select the name of feature columns A, C and D.\n",
    "        If None, returns all columns in the array.\n",
    "    drop_axis : bool (default=False)\n",
    "        Drops last axis if True and the only one column is selected. This\n",
    "        is useful, e.g., when the ColumnSelector is used for selecting\n",
    "        only one column and the resulting array should be fed to e.g.,\n",
    "        a scikit-learn column selector. E.g., instead of returning an\n",
    "        array with shape (n_samples, 1), drop_axis=True will return an\n",
    "        aray with shape (n_samples,).\n",
    "    Examples\n",
    "    -----------\n",
    "    For usage examples, please see\n",
    "    http://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cols=None, drop_axis=False):\n",
    "        self.cols = cols\n",
    "        self.drop_axis = drop_axis\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\" Return a slice of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_slice : shape = [n_samples, k_features]\n",
    "            Subset of the feature space where k_features <= n_features\n",
    "        \"\"\"\n",
    "        return self.transform(X=X, y=y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\" Return a slice of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_slice : shape = [n_samples, k_features]\n",
    "            Subset of the feature space where k_features <= n_features\n",
    "        \"\"\"\n",
    "\n",
    "        # We use the loc or iloc accessor if the input is a pandas dataframe\n",
    "        if hasattr(X, 'loc') or hasattr(X, 'iloc'):\n",
    "            if type(self.cols) == tuple:\n",
    "                self.cols = list(self.cols)\n",
    "            types = {type(i) for i in self.cols}\n",
    "            if len(types) > 1:\n",
    "                raise ValueError(\n",
    "                    'Elements in `cols` should be all of the same data type.'\n",
    "                )\n",
    "            if isinstance(self.cols[0], int):\n",
    "                t = X.iloc[:, self.cols].values\n",
    "            elif isinstance(self.cols[0], str):\n",
    "                t = X.loc[:, self.cols].values\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Elements in `cols` should be either `int` or `str`.'\n",
    "                )\n",
    "        else:\n",
    "            t = X[:, self.cols]\n",
    "\n",
    "        if t.shape[-1] == 1 and self.drop_axis:\n",
    "            t = t.reshape(-1)\n",
    "        if len(t.shape) == 1 and not self.drop_axis:\n",
    "            t = t[:, np.newaxis]\n",
    "        return t\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Mock method. Does nothing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        self\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "#import mlxtend\n",
    "#pipe1 = make_pipeline(ColumnSelector(cols=(7,)), MeanEmbeddingVectorizer2(fasttext2), LogisticRegression(\"l1\", random_state=0))\n",
    "#pipe2 = make_pipeline(ColumnSelector(cols=(5, )), TfidfEmbeddingVectorizer(fasttext), SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))\n",
    "\n",
    "#sclf = StackingClassifier(classifiers=[pipe1, pipe2], \n",
    "                         # meta_classifier=LogisticRegression())\n",
    "# Fit ensemble\n",
    "#sclf.fit(sample_df[0:1638], sample_df.label[0:1638].values)\n",
    "\n",
    "# Predict\n",
    "#preds = sclf.predict(sample_df[1639:])\n",
    "\n",
    "#accuracy=accuracy_score(sample_df.label[1639:], preds)\n",
    "#print(accuracy)\n",
    "\n",
    "log_reg_fasttext_tfidf = Pipeline([(\"col_sel\", ColumnSelector(cols=5, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer(fasttext)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "\n",
    "log_reg_fasttext2 = Pipeline([(\"col_sel\", ColumnSelector(cols=5, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer(fasttext)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "\n",
    "svm_fasttext = Pipeline([(\"col_sel\", ColumnSelector(cols=5, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer(fasttext)), \n",
    "                            (\"LinearSVC\", SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))])\n",
    "\n",
    "\n",
    "log_reg_fasttext_tfidf2 = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer2(fasttext2)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "#pipe_rf = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer2(fasttext2)),\n",
    "#                        ('clf', RandomForestClassifier(n_estimators = 140, max_features = 60, max_depth =120,\n",
    "#                                criterion = \"gini\",min_samples_split = 5, min_samples_leaf= 2,\n",
    "#                                                       random_state=0))])\n",
    "\n",
    "svm_fasttext_tfidf = Pipeline([(\"col_sel\", ColumnSelector(cols=5, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer(fasttext)), \n",
    "                            (\"LinearSVC\", SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikaterinikatsarou/homebrew_high_sierra/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7555012224938875\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "seed = 0\n",
    "#np.random.seed(seed)\n",
    "ensemble = SuperLearner(scorer=metrics.accuracy_score, random_state=seed)\n",
    "\n",
    "# Build the first -rflayer\n",
    "ensemble.add([svm_fasttext, log_reg_fasttext_tfidf2,log_reg_fasttext2, svm_fasttext_tfidf] )\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(LogisticRegression(\"l2\", random_state=0))\n",
    "# --- Use ---\n",
    "\n",
    "# Fit ensemble\n",
    "ensemble.fit(sample_df[0:1638].values, sample_df.label[0:1638].values)\n",
    "\n",
    "# Predict\n",
    "preds = ensemble.predict(sample_df[1638:].values)\n",
    "\n",
    "accuracy=accuracy_score(sample_df.label[1638:].values, preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c589eab13420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_df' is not defined"
     ]
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
