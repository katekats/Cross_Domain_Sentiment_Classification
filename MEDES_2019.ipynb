{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#import json\n",
    "#import optparse\n",
    "import os, regex as re\n",
    "import pandas as pd\n",
    "\n",
    "#import libraries \n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re \n",
    "#import matplotlib.pyplot as plt \n",
    "#import seaborn as sns\n",
    "import scipy as sp\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='Downloads/processed_acl/books/negative.review' mode='r+' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='Downloads/processed_acl/books/positive.review' mode='r+' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='Downloads/processed_acl/dvd/negative.review' mode='r+' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='Downloads/processed_acl/dvd/positive.review' mode='r+' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='Downloads/processed_acl/kitchen/negative.review' mode='r+' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='Downloads/processed_acl/kitchen/positive.review' mode='r+' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='Downloads/processed_acl/electronics/negative.review' mode='r+' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='Downloads/processed_acl/electronics/positive.review' mode='r+' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    list1 = []\n",
    "    reviews = []\n",
    "    with open(filename, 'r+') as fr:\n",
    "        print(fr)\n",
    "        end_of_review = 0\n",
    "        for line in fr:\n",
    "            #print(\"is not empty\")\n",
    "            line = re.sub(r'[:][\\d]', \" \", str(line))\n",
    "            if (re.search(\"#label#:negative\", str(line))):\n",
    "                line = re.sub(\"#label#:negative\", \" \" ,str(line))\n",
    "                end_of_review=1\n",
    "            if (re.search(\"#label#:positive\", str(line))):\n",
    "                line = re.sub(\"#label#:positive\", \" \" ,str(line))\n",
    "                end_of_review=1    \n",
    "            str1 = str(line)    \n",
    "                #print(\"end-of review\")\n",
    "            if end_of_review == 1:    \n",
    "                reviews.append(str1)\n",
    "                end_of_review = 0           \n",
    "           \n",
    "        return (reviews)\n",
    "            #print(list1)      \n",
    "        \n",
    "def convert_to_dataframe(listname):\n",
    "    df1 = pd.DataFrame({'reviews':listname})\n",
    "    return df1\n",
    "\n",
    "def get_label_from_filename(filename, df):\n",
    "    if re.search(\"positive\", str(filename)):\n",
    "        df[\"label\"] = 1\n",
    "    if  re.search(\"negative\", str(filename)):\n",
    "        df[\"label\"] = 0 \n",
    "        #pd.set_option('display.max_colwidth', -1)\n",
    "    return df   \n",
    "\n",
    "# books_dataset\n",
    "neg_reviews_list = read_data('Downloads/processed_acl/books/negative.review')\n",
    "df1 = convert_to_dataframe(neg_reviews_list)\n",
    "df1 =get_label_from_filename('Downloads/processed_acl/books/negative.review', df1)\n",
    "\n",
    "\n",
    "pos_reviews_list = read_data('Downloads/processed_acl/books/positive.review')\n",
    "df2 = convert_to_dataframe(pos_reviews_list)\n",
    "df2 =get_label_from_filename('Downloads/processed_acl/books/positive.review', df2)\n",
    "\n",
    "\n",
    "df_books = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# dvd_dataset\n",
    "neg_reviews_list2 = read_data('Downloads/processed_acl/dvd/negative.review')\n",
    "df3 = convert_to_dataframe(neg_reviews_list2)\n",
    "df3 =get_label_from_filename('Downloads/processed_acl/dvd/negative.review', df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pos_reviews_list2 = read_data('Downloads/processed_acl/dvd/positive.review')\n",
    "df4 = convert_to_dataframe(pos_reviews_list2)\n",
    "df4 =get_label_from_filename('Downloads/processed_acl/dvd/positive.review', df4)\n",
    "\n",
    "df_dvd = pd.concat([df3, df4], axis=0)\n",
    "\n",
    "# kitchen_dataset\n",
    "neg_reviews_list3 = read_data('Downloads/processed_acl/kitchen/negative.review')\n",
    "df5 = convert_to_dataframe(neg_reviews_list3)\n",
    "df5 =get_label_from_filename('Downloads/processed_acl/kitchen/negative.review', df5)\n",
    "\n",
    "\n",
    "pos_reviews_list3 = read_data('Downloads/processed_acl/kitchen/positive.review')\n",
    "df6 = convert_to_dataframe(pos_reviews_list3)\n",
    "df6 =get_label_from_filename('Downloads/processed_acl/kitchen/positive.review', df6)\n",
    "\n",
    "df_kitchen = pd.concat([df5, df6], axis=0)\n",
    "\n",
    "# electronics_dataset\n",
    "neg_reviews_list4 = read_data('Downloads/processed_acl/electronics/negative.review')\n",
    "df7 = convert_to_dataframe(neg_reviews_list4)\n",
    "df7 =get_label_from_filename('Downloads/processed_acl/electronics/negative.review', df7)\n",
    "\n",
    "\n",
    "pos_reviews_list4 = read_data('Downloads/processed_acl/electronics/positive.review')\n",
    "df8 = convert_to_dataframe(pos_reviews_list4)\n",
    "df8 =get_label_from_filename('Downloads/processed_acl/electronics/positive.review', df8)\n",
    "\n",
    "df_electronics = pd.concat([df7, df8], axis=0)\n",
    "\n",
    "\n",
    "#adding column for number of words in review in original data frame\n",
    "df_books['#words'] = df_books.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "df_dvd['#words'] = df_dvd.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "#e_df['#words'] = e_df.reviewText.apply(lambda x: len(str(x).split(' ')))\n",
    "#k_df['#words'] = k_df.reviewText.apply(lambda x: len(str(x).split(' ')))\n",
    "df_kitchen['#words'] = df_kitchen.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "df_electronics['#words'] = df_electronics.apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "#Shuffling the rows in all the datasets to make them randomly ordered\n",
    "df_books.sample(frac=1)\n",
    "df_books = df_books.sample(frac=1).reset_index(drop=True)\n",
    "df_books[\"code\"] = \"books\"\n",
    "\n",
    "df_dvd.sample(frac=1)\n",
    "df_dvd = df_dvd.sample(frac=1).reset_index(drop=True)\n",
    "df_dvd[\"code\"] = \"dvd\"\n",
    "\n",
    "df_kitchen.sample(frac=1)\n",
    "df_kitchen = df_kitchen.sample(frac=1).reset_index(drop=True)\n",
    "df_kitchen[\"code\"] = \"kitchen\"\n",
    "\n",
    "df_electronics.sample(frac=1)\n",
    "df_electronics = df_electronics.sample(frac=1).reset_index(drop=True)\n",
    "df_electronics[\"code\"] = \"electronics\"\n",
    "\n",
    "#Appending the datasets CDSA \n",
    "bd = df_books.append(df_dvd, ignore_index=True)\n",
    "bk = df_books.append(df_kitchen, ignore_index=True)\n",
    "db = df_dvd.append(df_books, ignore_index=True)\n",
    "eb = df_electronics.append(df_books, ignore_index=True)\n",
    "kb = df_kitchen.append(df_books, ignore_index=True)\n",
    "ed = df_electronics.append(df_dvd, ignore_index=True)\n",
    "kd = df_kitchen.append(df_dvd, ignore_index=True)\n",
    "be = df_books.append(df_electronics, ignore_index=True)\n",
    "de = df_dvd.append(df_electronics, ignore_index=True)\n",
    "ke = df_kitchen.append(df_electronics, ignore_index=True)\n",
    "ek = df_electronics.append(df_kitchen, ignore_index=True)\n",
    "dk = df_dvd.append(df_kitchen, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_dvd\n",
    "#Functions for preprocessing steps\n",
    "stop = set(('i','im','ive', 'me','my','myself','we','our','ours','ourselves','you','youre','youve','youll','youd','your','yours','yourself','yourselves','he','him','his','himself','she','shes','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','thatll','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','only','own','same','so','than','too','very','s','t','can','will','just','should','shouldve','now','d','ll','m','o','re','ve','y','ma'))\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def replace_url(df,col,rm1,rm2):\n",
    "    return(df[col].str.replace(rm1,rm2))\n",
    "\n",
    "def extract_emo(df, col, emo):\n",
    "    return(df[col].str.extractall(emo).unstack().apply(lambda x:' '.join(x.dropna()), axis=1))\n",
    "\n",
    "def replace_emo(df,col,emo1,emo2):\n",
    "    return(df[col].str.replace(emo1,emo2))\n",
    "\n",
    "def replace_punct(df, col, punct1, punct2):\n",
    "    return(df[col].str.replace(punct1, punct2))\n",
    "\n",
    "def remove_numbers(df,col,rm1,rm2):\n",
    "    return(df[col].str.replace(rm1,rm2))\n",
    "\n",
    "def lower_words(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join(x.lower() for x in x.split())))\n",
    "\n",
    "def remove_stop(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)))\n",
    "\n",
    "def tokenize(df,col):\n",
    "    return(df.apply(lambda row: nltk.word_tokenize(row[col]), axis=1))\n",
    "\n",
    "def word_count(df,col):\n",
    "    return(df[col].apply(lambda x: len(str(x).split(' '))))\n",
    "\n",
    "def stemming(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join([sno.stem(word) for word in x.split()])))\n",
    "\n",
    "\n",
    "#Step1 Pre-Processing\n",
    "sample_df['nohtml'] = replace_url(sample_df,'reviews','^http?:\\/\\/.*[\\r\\n]*','')\n",
    "sample_df['nohtml'] = lower_words(sample_df,'nohtml')\n",
    "sample_df['nohtml'] = remove_numbers(sample_df, 'nohtml', '[0-9]+',' ')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml', '[^\\w\\s]',' ')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml', '_',' ')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml',r'\\b(no|not|nt|dont|doesnt|doesn|don|didnt|cant|cannt|cannot|wouldnt|wont|couldnt|hasnt|havent|hadnt|shouldnt)\\s+([a-z])',r'not \\2')\n",
    "sample_df['nohtml'] = remove_stop(sample_df,'nohtml')\n",
    "#sample_df['nohtml'] = stemming(sample_df,'nohtml')\n",
    "sample_df['tokenized'] = tokenize(sample_df,'nohtml')\n",
    "sample_df['#token'] = word_count(sample_df,'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_dvd =sample_df[sample_df[\"#token\"]>75].reset_index(drop=True)\n",
    "#sample_df_dvd =sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_books=sample_df[sample_df[\"#token\"]>80].reset_index(drop=True)\n",
    "#sample_df_books =sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_electronics=sample_df[sample_df[\"#token\"]>=55].reset_index(drop=True)\n",
    "#sample_df_electronics = sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_kitchen=sample_df[sample_df[\"#token\"]>=54].reset_index(drop=True)\n",
    "#sample_df_kitchen = sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending the datasets CDSA \n",
    "bd = sample_df_books.append(sample_df_dvd, ignore_index=True)\n",
    "bk = sample_df_books.append(sample_df_kitchen, ignore_index=True)\n",
    "db = sample_df_dvd.append(sample_df_books, ignore_index=True)\n",
    "eb = sample_df_electronics.append(sample_df_books, ignore_index=True)\n",
    "kb = sample_df_kitchen.append(sample_df_books, ignore_index=True)\n",
    "ed = sample_df_electronics.append(sample_df_dvd, ignore_index=True)\n",
    "kd = sample_df_kitchen.append(sample_df_dvd, ignore_index=True)\n",
    "be = sample_df_books.append(sample_df_electronics, ignore_index=True)\n",
    "de = sample_df_dvd.append(sample_df_electronics, ignore_index=True)\n",
    "ke = sample_df_kitchen.append(sample_df_electronics, ignore_index=True)\n",
    "ek = sample_df_electronics.append(sample_df_kitchen, ignore_index=True)\n",
    "dk = sample_df_dvd.append(sample_df_kitchen, ignore_index=True)\n",
    "sample_df1=db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>label</th>\n",
       "      <th>#words</th>\n",
       "      <th>code</th>\n",
       "      <th>nohtml</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>#token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the_gateway  only  why).there  help  like  one...</td>\n",
       "      <td>0</td>\n",
       "      <td>842.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>gateway help like one women ever trying women ...</td>\n",
       "      <td>[gateway, help, like, one, women, ever, trying...</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from_morris  fans_will  bomb_why  the_bomb  wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>250.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>morris fans bomb bomb love day true disappoint...</td>\n",
       "      <td>[morris, fans, bomb, bomb, love, day, true, di...</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the_entire  school's  time_to  worst  i_say  m...</td>\n",
       "      <td>0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>entire school time worst say meteroid acting s...</td>\n",
       "      <td>[entire, school, time, worst, say, meteroid, a...</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film  found  both  open_minded  making.....the...</td>\n",
       "      <td>0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>film found open minded making whole director w...</td>\n",
       "      <td>[film, found, open, minded, making, whole, dir...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>story_but  the_editing  a_dvd  transfer_was  t...</td>\n",
       "      <td>0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>story editing dvd transfer quality movies noti...</td>\n",
       "      <td>[story, editing, dvd, transfer, quality, movie...</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>undo  awry_they  middle  neighbor  they're_gro...</td>\n",
       "      <td>0</td>\n",
       "      <td>778.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>undo awry middle neighbor grown times leaves s...</td>\n",
       "      <td>[undo, awry, middle, neighbor, grown, times, l...</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>often_makes  is_simply  ties_the  create  rema...</td>\n",
       "      <td>0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>often makes simply ties create remaining half ...</td>\n",
       "      <td>[often, makes, simply, ties, create, remaining...</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>anyone  relentless  not  you_agree  of_them  n...</td>\n",
       "      <td>0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>anyone relentless not agree news station howev...</td>\n",
       "      <td>[anyone, relentless, not, agree, news, station...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>well  reminded_me  special  well_as  warning  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>well reminded special well warning broadcasts ...</td>\n",
       "      <td>[well, reminded, special, well, warning, broad...</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scenery_is  very_decent  sigur�sson_as  tellin...</td>\n",
       "      <td>1</td>\n",
       "      <td>172.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>scenery decent sigur sson telling thoughts sim...</td>\n",
       "      <td>[scenery, decent, sigur, sson, telling, though...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>only  he_is  the_rating  black_screen  present...</td>\n",
       "      <td>1</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>rating black screen presentation eyal yanilov ...</td>\n",
       "      <td>[rating, black, screen, presentation, eyal, ya...</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>millions=b++_growing  pains=a++_love  a_croc=b...</td>\n",
       "      <td>1</td>\n",
       "      <td>426.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>millions b growing pains love croc b new robin...</td>\n",
       "      <td>[millions, b, growing, pains, love, croc, b, n...</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>this_documentary  comments  film  greatest_dir...</td>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>documentary comments film greatest director ku...</td>\n",
       "      <td>[documentary, comments, film, greatest, direct...</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>star_of  male_and  ignorant  betty  this_becau...</td>\n",
       "      <td>1</td>\n",
       "      <td>414.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>star male ignorant betty older watch star ugly...</td>\n",
       "      <td>[star, male, ignorant, betty, older, watch, st...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>leading_participant  downright  favorite  alls...</td>\n",
       "      <td>0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>leading participant downright favorite allsber...</td>\n",
       "      <td>[leading, participant, downright, favorite, al...</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>we_all  live  artist_her  dynamics  it_some  k...</td>\n",
       "      <td>1</td>\n",
       "      <td>390.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>live artist dynamics keep deep feelings movie ...</td>\n",
       "      <td>[live, artist, dynamics, keep, deep, feelings,...</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>capitalize_on  is_simply  you_the  at_all  bas...</td>\n",
       "      <td>0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>capitalize simply basically van okay big think...</td>\n",
       "      <td>[capitalize, simply, basically, van, okay, big...</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pay  think_it  then_i  boondock  heard  though...</td>\n",
       "      <td>1</td>\n",
       "      <td>222.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>pay think boondock heard though though adore p...</td>\n",
       "      <td>[pay, think, boondock, heard, though, though, ...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>workout  &lt;num&gt;_minute  first  are_still  spot_...</td>\n",
       "      <td>1</td>\n",
       "      <td>172.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>workout num minute first still spot wrap still...</td>\n",
       "      <td>[workout, num, minute, first, still, spot, wra...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>coletrane_and  death_reminds  louis  including...</td>\n",
       "      <td>0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>coletrane death reminds louis including rebut ...</td>\n",
       "      <td>[coletrane, death, reminds, louis, including, ...</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>camp_life  fan_and  well  survivor  reason  di...</td>\n",
       "      <td>1</td>\n",
       "      <td>386.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>camp life fan well survivor reason give speaci...</td>\n",
       "      <td>[camp, life, fan, well, survivor, reason, give...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>amount_of  well  nearly_as  how_awesome  adven...</td>\n",
       "      <td>1</td>\n",
       "      <td>224.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>amount well nearly awesome adventure style see...</td>\n",
       "      <td>[amount, well, nearly, awesome, adventure, sty...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>this_guy  in_his  directed_the  match  least_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>guy directed match least sense murderer liked ...</td>\n",
       "      <td>[guy, directed, match, least, sense, murderer,...</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sort  to_use  looking_at  it_doesn't)  \"save\"_...</td>\n",
       "      <td>0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>sort use looking not save num someone else fiv...</td>\n",
       "      <td>[sort, use, looking, not, save, num, someone, ...</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>all_pretty  film  the_acting  as_i  ten  cost ...</td>\n",
       "      <td>0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>pretty film acting ten cost considering prospe...</td>\n",
       "      <td>[pretty, film, acting, ten, cost, considering,...</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>therefore_i  recommend_it  film  his_life  con...</td>\n",
       "      <td>1</td>\n",
       "      <td>310.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>therefore recommend film life constant mystery...</td>\n",
       "      <td>[therefore, recommend, film, life, constant, m...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>owe_it  casts  &lt;dash-num&gt;  season_sets  out_bo...</td>\n",
       "      <td>1</td>\n",
       "      <td>402.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>owe casts dash num season sets comedy second d...</td>\n",
       "      <td>[owe, casts, dash, num, season, sets, comedy, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>played  farrow_doesn't  loser  trust_edwina  i...</td>\n",
       "      <td>1</td>\n",
       "      <td>250.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>played farrow not loser trust edwina worth tru...</td>\n",
       "      <td>[played, farrow, not, loser, trust, edwina, wo...</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>babies_and  identifying_the  tv  10-month-old ...</td>\n",
       "      <td>1</td>\n",
       "      <td>508.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>babies identifying tv month old second time pa...</td>\n",
       "      <td>[babies, identifying, tv, month, old, second, ...</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>say...\"the  playing  all...just_watch  playing...</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>say playing watch playing ideas lost humbler k...</td>\n",
       "      <td>[say, playing, watch, playing, ideas, lost, hu...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>was_kind  your  bus_and  of_annoying  footage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>304.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>kind bus annoying footage annoying incubus dif...</td>\n",
       "      <td>[kind, bus, annoying, footage, annoying, incub...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>your  beginning_students  moving_in  in_very  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>beginning students moving workout routine real...</td>\n",
       "      <td>[beginning, students, moving, workout, routine...</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>on_paper  mel  he  him_to  jodie  and_beyond  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>paper mel jodie beyond sequence jodie nice seq...</td>\n",
       "      <td>[paper, mel, jodie, beyond, sequence, jodie, n...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>sort  mass_suicide  the_entire  writer-directo...</td>\n",
       "      <td>0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>sort mass suicide entire writer director gerw ...</td>\n",
       "      <td>[sort, mass, suicide, entire, writer, director...</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>the_music  the_dvd  a*teens_music  get  bonus ...</td>\n",
       "      <td>0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>music dvd teens music get bonus lemme know dvd...</td>\n",
       "      <td>[music, dvd, teens, music, get, bonus, lem, me...</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>&lt;num&gt;_toronto  time_machine  a_historial  one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>num toronto time machine historial one damn ch...</td>\n",
       "      <td>[num, toronto, time, machine, historial, one, ...</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>incredibly  your  i_do  next_day  this_video  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>282.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>incredibly next day video great dvd minutes in...</td>\n",
       "      <td>[incredibly, next, day, video, great, dvd, min...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>made_this  the_idea  the_acting  is_pathetic  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>made idea acting pathetic pathetic act movie y...</td>\n",
       "      <td>[made, idea, acting, pathetic, pathetic, act, ...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>i  he  from_someone  going  jack  and_shot  se...</td>\n",
       "      <td>1</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>someone going jack shot sell perfectly violent...</td>\n",
       "      <td>[someone, going, jack, shot, sell, perfectly, ...</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>swimming_to  and_uninspired  didnt_like  none ...</td>\n",
       "      <td>0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>swimming uninspired not like none joint three ...</td>\n",
       "      <td>[swimming, uninspired, not, like, none, joint,...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>i  cast  theatres_when  came  came_out  though...</td>\n",
       "      <td>1</td>\n",
       "      <td>184.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>cast theatres came came thoughtly modern promi...</td>\n",
       "      <td>[cast, theatres, came, came, thoughtly, modern...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>w  &lt;num&gt;_segments  well  learn  min_bonus  alt...</td>\n",
       "      <td>1</td>\n",
       "      <td>392.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>w num segments well learn min bonus alternativ...</td>\n",
       "      <td>[w, num, segments, well, learn, min, bonus, al...</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>on_something  innocence  double  about  not_be...</td>\n",
       "      <td>0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>something innocence double not better innocenc...</td>\n",
       "      <td>[something, innocence, double, not, better, in...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>ends_up  been_more  sexual_longing  wondrous_a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>ends sexual longing wondrous hingle onscreen w...</td>\n",
       "      <td>[ends, sexual, longing, wondrous, hingle, onsc...</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>late-night  your  well  gets  well_done  shows...</td>\n",
       "      <td>0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>late night well gets well done shows shows res...</td>\n",
       "      <td>[late, night, well, gets, well, done, shows, s...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>i  pride_  like  rather_than  shown  captain's...</td>\n",
       "      <td>0</td>\n",
       "      <td>860.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>pride like rather shown captain constant preju...</td>\n",
       "      <td>[pride, like, rather, shown, captain, constant...</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>dvd_takes  quite_well  the_dvd  well_and  reco...</td>\n",
       "      <td>1</td>\n",
       "      <td>158.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>dvd takes quite well dvd well recommend funnie...</td>\n",
       "      <td>[dvd, takes, quite, well, dvd, well, recommend...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>language_ideas  the_whole  and_unconvincing  s...</td>\n",
       "      <td>0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>language ideas whole unconvincing seems absurd...</td>\n",
       "      <td>[language, ideas, whole, unconvincing, seems, ...</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>and_never  film_was  wonderful_if  like  thems...</td>\n",
       "      <td>1</td>\n",
       "      <td>918.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>never film wonderful like personal possibly fo...</td>\n",
       "      <td>[never, film, wonderful, like, personal, possi...</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>connected  la  one  musique  early_sound  futu...</td>\n",
       "      <td>1</td>\n",
       "      <td>952.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>connected la one musique early sound futuristi...</td>\n",
       "      <td>[connected, la, one, musique, early, sound, fu...</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>0f  version_i  including  right_dolby  the_tra...</td>\n",
       "      <td>1</td>\n",
       "      <td>464.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>f version including right dolby transfer vhs e...</td>\n",
       "      <td>[f, version, including, right, dolby, transfer...</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>unsympathetic.yes  glover  me  other's_compani...</td>\n",
       "      <td>1</td>\n",
       "      <td>540.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>unsympathetic yes glover companionship group t...</td>\n",
       "      <td>[unsympathetic, yes, glover, companionship, gr...</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>\"coraline.\"_but  well_it's  like  there_might ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1096.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>coraline well like might going yet creeped sto...</td>\n",
       "      <td>[coraline, well, like, might, going, yet, cree...</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>to_see  a_nobody  acting_is  so  so_horrible  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>746.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>see nobody acting horrible funny speak becomin...</td>\n",
       "      <td>[see, nobody, acting, horrible, funny, speak, ...</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>one_more  was..well..stupid  the_whole  it_&lt;nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>one well stupid whole num new venture sigourne...</td>\n",
       "      <td>[one, well, stupid, whole, num, new, venture, ...</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>i  the_dvd  to_write  maybe_this  read  three ...</td>\n",
       "      <td>0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>dvd write maybe read three fox dvd sure make i...</td>\n",
       "      <td>[dvd, write, maybe, read, three, fox, dvd, sur...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>have_added  field_trip  on_birds  of_color  lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>added field trip birds color longer display bi...</td>\n",
       "      <td>[added, field, trip, birds, color, longer, dis...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>a_poor  doesn't_grab  minutes_and  she_was  my...</td>\n",
       "      <td>0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>poor not grab minutes barely understand grab t...</td>\n",
       "      <td>[poor, not, grab, minutes, barely, understand,...</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>the_sequels  must  stimpy  i_kept  send  socia...</td>\n",
       "      <td>0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>sequels must stimpy kept send social message m...</td>\n",
       "      <td>[sequels, must, stimpy, kept, send, social, me...</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>mean  still_highly  at_all  right_but  so_for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>606.0</td>\n",
       "      <td>dvd</td>\n",
       "      <td>mean still highly right still role got tremend...</td>\n",
       "      <td>[mean, still, highly, right, still, role, got,...</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1637 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  label  #words code  \\\n",
       "0     the_gateway  only  why).there  help  like  one...      0   842.0  dvd   \n",
       "1     from_morris  fans_will  bomb_why  the_bomb  wi...      1   250.0  dvd   \n",
       "2     the_entire  school's  time_to  worst  i_say  m...      0   514.0  dvd   \n",
       "3     film  found  both  open_minded  making.....the...      0   300.0  dvd   \n",
       "4     story_but  the_editing  a_dvd  transfer_was  t...      0   700.0  dvd   \n",
       "5     undo  awry_they  middle  neighbor  they're_gro...      0   778.0  dvd   \n",
       "6     often_makes  is_simply  ties_the  create  rema...      0   606.0  dvd   \n",
       "7     anyone  relentless  not  you_agree  of_them  n...      0   358.0  dvd   \n",
       "8     well  reminded_me  special  well_as  warning  ...      0   242.0  dvd   \n",
       "9     scenery_is  very_decent  sigur�sson_as  tellin...      1   172.0  dvd   \n",
       "10    only  he_is  the_rating  black_screen  present...      1  1162.0  dvd   \n",
       "11    millions=b++_growing  pains=a++_love  a_croc=b...      1   426.0  dvd   \n",
       "12    this_documentary  comments  film  greatest_dir...      1   324.0  dvd   \n",
       "13    star_of  male_and  ignorant  betty  this_becau...      1   414.0  dvd   \n",
       "14    leading_participant  downright  favorite  alls...      0   528.0  dvd   \n",
       "15    we_all  live  artist_her  dynamics  it_some  k...      1   390.0  dvd   \n",
       "16    capitalize_on  is_simply  you_the  at_all  bas...      0   640.0  dvd   \n",
       "17    pay  think_it  then_i  boondock  heard  though...      1   222.0  dvd   \n",
       "18    workout  <num>_minute  first  are_still  spot_...      1   172.0  dvd   \n",
       "19    coletrane_and  death_reminds  louis  including...      0   476.0  dvd   \n",
       "20    camp_life  fan_and  well  survivor  reason  di...      1   386.0  dvd   \n",
       "21    amount_of  well  nearly_as  how_awesome  adven...      1   224.0  dvd   \n",
       "22    this_guy  in_his  directed_the  match  least_s...      0   428.0  dvd   \n",
       "23    sort  to_use  looking_at  it_doesn't)  \"save\"_...      0   524.0  dvd   \n",
       "24    all_pretty  film  the_acting  as_i  ten  cost ...      0   280.0  dvd   \n",
       "25    therefore_i  recommend_it  film  his_life  con...      1   310.0  dvd   \n",
       "26    owe_it  casts  <dash-num>  season_sets  out_bo...      1   402.0  dvd   \n",
       "27    played  farrow_doesn't  loser  trust_edwina  i...      1   250.0  dvd   \n",
       "28    babies_and  identifying_the  tv  10-month-old ...      1   508.0  dvd   \n",
       "29    say...\"the  playing  all...just_watch  playing...      1   154.0  dvd   \n",
       "...                                                 ...    ...     ...  ...   \n",
       "1607  was_kind  your  bus_and  of_annoying  footage ...      1   304.0  dvd   \n",
       "1608  your  beginning_students  moving_in  in_very  ...      0   256.0  dvd   \n",
       "1609  on_paper  mel  he  him_to  jodie  and_beyond  ...      0   546.0  dvd   \n",
       "1610  sort  mass_suicide  the_entire  writer-directo...      0   564.0  dvd   \n",
       "1611  the_music  the_dvd  a*teens_music  get  bonus ...      0   180.0  dvd   \n",
       "1612  <num>_toronto  time_machine  a_historial  one ...      0  1004.0  dvd   \n",
       "1613  incredibly  your  i_do  next_day  this_video  ...      1   282.0  dvd   \n",
       "1614  made_this  the_idea  the_acting  is_pathetic  ...      0   216.0  dvd   \n",
       "1615  i  he  from_someone  going  jack  and_shot  se...      1  2404.0  dvd   \n",
       "1616  swimming_to  and_uninspired  didnt_like  none ...      0   272.0  dvd   \n",
       "1617  i  cast  theatres_when  came  came_out  though...      1   184.0  dvd   \n",
       "1618  w  <num>_segments  well  learn  min_bonus  alt...      1   392.0  dvd   \n",
       "1619  on_something  innocence  double  about  not_be...      0   192.0  dvd   \n",
       "1620  ends_up  been_more  sexual_longing  wondrous_a...      1  1132.0  dvd   \n",
       "1621  late-night  your  well  gets  well_done  shows...      0   212.0  dvd   \n",
       "1622  i  pride_  like  rather_than  shown  captain's...      0   860.0  dvd   \n",
       "1623  dvd_takes  quite_well  the_dvd  well_and  reco...      1   158.0  dvd   \n",
       "1624  language_ideas  the_whole  and_unconvincing  s...      0   410.0  dvd   \n",
       "1625  and_never  film_was  wonderful_if  like  thems...      1   918.0  dvd   \n",
       "1626  connected  la  one  musique  early_sound  futu...      1   952.0  dvd   \n",
       "1627  0f  version_i  including  right_dolby  the_tra...      1   464.0  dvd   \n",
       "1628  unsympathetic.yes  glover  me  other's_compani...      1   540.0  dvd   \n",
       "1629  \"coraline.\"_but  well_it's  like  there_might ...      1  1096.0  dvd   \n",
       "1630  to_see  a_nobody  acting_is  so  so_horrible  ...      0   746.0  dvd   \n",
       "1631  one_more  was..well..stupid  the_whole  it_<nu...      0   470.0  dvd   \n",
       "1632  i  the_dvd  to_write  maybe_this  read  three ...      0   158.0  dvd   \n",
       "1633  have_added  field_trip  on_birds  of_color  lo...      0   222.0  dvd   \n",
       "1634  a_poor  doesn't_grab  minutes_and  she_was  my...      0   252.0  dvd   \n",
       "1635  the_sequels  must  stimpy  i_kept  send  socia...      0   542.0  dvd   \n",
       "1636  mean  still_highly  at_all  right_but  so_for ...      1   606.0  dvd   \n",
       "\n",
       "                                                 nohtml  \\\n",
       "0     gateway help like one women ever trying women ...   \n",
       "1     morris fans bomb bomb love day true disappoint...   \n",
       "2     entire school time worst say meteroid acting s...   \n",
       "3     film found open minded making whole director w...   \n",
       "4     story editing dvd transfer quality movies noti...   \n",
       "5     undo awry middle neighbor grown times leaves s...   \n",
       "6     often makes simply ties create remaining half ...   \n",
       "7     anyone relentless not agree news station howev...   \n",
       "8     well reminded special well warning broadcasts ...   \n",
       "9     scenery decent sigur sson telling thoughts sim...   \n",
       "10    rating black screen presentation eyal yanilov ...   \n",
       "11    millions b growing pains love croc b new robin...   \n",
       "12    documentary comments film greatest director ku...   \n",
       "13    star male ignorant betty older watch star ugly...   \n",
       "14    leading participant downright favorite allsber...   \n",
       "15    live artist dynamics keep deep feelings movie ...   \n",
       "16    capitalize simply basically van okay big think...   \n",
       "17    pay think boondock heard though though adore p...   \n",
       "18    workout num minute first still spot wrap still...   \n",
       "19    coletrane death reminds louis including rebut ...   \n",
       "20    camp life fan well survivor reason give speaci...   \n",
       "21    amount well nearly awesome adventure style see...   \n",
       "22    guy directed match least sense murderer liked ...   \n",
       "23    sort use looking not save num someone else fiv...   \n",
       "24    pretty film acting ten cost considering prospe...   \n",
       "25    therefore recommend film life constant mystery...   \n",
       "26    owe casts dash num season sets comedy second d...   \n",
       "27    played farrow not loser trust edwina worth tru...   \n",
       "28    babies identifying tv month old second time pa...   \n",
       "29    say playing watch playing ideas lost humbler k...   \n",
       "...                                                 ...   \n",
       "1607  kind bus annoying footage annoying incubus dif...   \n",
       "1608  beginning students moving workout routine real...   \n",
       "1609  paper mel jodie beyond sequence jodie nice seq...   \n",
       "1610  sort mass suicide entire writer director gerw ...   \n",
       "1611  music dvd teens music get bonus lemme know dvd...   \n",
       "1612  num toronto time machine historial one damn ch...   \n",
       "1613  incredibly next day video great dvd minutes in...   \n",
       "1614  made idea acting pathetic pathetic act movie y...   \n",
       "1615  someone going jack shot sell perfectly violent...   \n",
       "1616  swimming uninspired not like none joint three ...   \n",
       "1617  cast theatres came came thoughtly modern promi...   \n",
       "1618  w num segments well learn min bonus alternativ...   \n",
       "1619  something innocence double not better innocenc...   \n",
       "1620  ends sexual longing wondrous hingle onscreen w...   \n",
       "1621  late night well gets well done shows shows res...   \n",
       "1622  pride like rather shown captain constant preju...   \n",
       "1623  dvd takes quite well dvd well recommend funnie...   \n",
       "1624  language ideas whole unconvincing seems absurd...   \n",
       "1625  never film wonderful like personal possibly fo...   \n",
       "1626  connected la one musique early sound futuristi...   \n",
       "1627  f version including right dolby transfer vhs e...   \n",
       "1628  unsympathetic yes glover companionship group t...   \n",
       "1629  coraline well like might going yet creeped sto...   \n",
       "1630  see nobody acting horrible funny speak becomin...   \n",
       "1631  one well stupid whole num new venture sigourne...   \n",
       "1632  dvd write maybe read three fox dvd sure make i...   \n",
       "1633  added field trip birds color longer display bi...   \n",
       "1634  poor not grab minutes barely understand grab t...   \n",
       "1635  sequels must stimpy kept send social message m...   \n",
       "1636  mean still highly right still role got tremend...   \n",
       "\n",
       "                                              tokenized  #token  \n",
       "0     [gateway, help, like, one, women, ever, trying...     428  \n",
       "1     [morris, fans, bomb, bomb, love, day, true, di...     133  \n",
       "2     [entire, school, time, worst, say, meteroid, a...     257  \n",
       "3     [film, found, open, minded, making, whole, dir...     165  \n",
       "4     [story, editing, dvd, transfer, quality, movie...     365  \n",
       "5     [undo, awry, middle, neighbor, grown, times, l...     400  \n",
       "6     [often, makes, simply, ties, create, remaining...     332  \n",
       "7     [anyone, relentless, not, agree, news, station...     187  \n",
       "8     [well, reminded, special, well, warning, broad...     135  \n",
       "9     [scenery, decent, sigur, sson, telling, though...     103  \n",
       "10    [rating, black, screen, presentation, eyal, ya...     610  \n",
       "11    [millions, b, growing, pains, love, croc, b, n...     280  \n",
       "12    [documentary, comments, film, greatest, direct...     172  \n",
       "13    [star, male, ignorant, betty, older, watch, st...     192  \n",
       "14    [leading, participant, downright, favorite, al...     276  \n",
       "15    [live, artist, dynamics, keep, deep, feelings,...     202  \n",
       "16    [capitalize, simply, basically, van, okay, big...     311  \n",
       "17    [pay, think, boondock, heard, though, though, ...     109  \n",
       "18    [workout, num, minute, first, still, spot, wra...      83  \n",
       "19    [coletrane, death, reminds, louis, including, ...     255  \n",
       "20    [camp, life, fan, well, survivor, reason, give...     180  \n",
       "21    [amount, well, nearly, awesome, adventure, sty...     112  \n",
       "22    [guy, directed, match, least, sense, murderer,...     222  \n",
       "23    [sort, use, looking, not, save, num, someone, ...     310  \n",
       "24    [pretty, film, acting, ten, cost, considering,...     152  \n",
       "25    [therefore, recommend, film, life, constant, m...     148  \n",
       "26    [owe, casts, dash, num, season, sets, comedy, ...     200  \n",
       "27    [played, farrow, not, loser, trust, edwina, wo...     113  \n",
       "28    [babies, identifying, tv, month, old, second, ...     239  \n",
       "29    [say, playing, watch, playing, ideas, lost, hu...      84  \n",
       "...                                                 ...     ...  \n",
       "1607  [kind, bus, annoying, footage, annoying, incub...     154  \n",
       "1608  [beginning, students, moving, workout, routine...     123  \n",
       "1609  [paper, mel, jodie, beyond, sequence, jodie, n...     258  \n",
       "1610  [sort, mass, suicide, entire, writer, director...     290  \n",
       "1611  [music, dvd, teens, music, get, bonus, lem, me...      97  \n",
       "1612  [num, toronto, time, machine, historial, one, ...     486  \n",
       "1613  [incredibly, next, day, video, great, dvd, min...     127  \n",
       "1614  [made, idea, acting, pathetic, pathetic, act, ...     112  \n",
       "1615  [someone, going, jack, shot, sell, perfectly, ...    1489  \n",
       "1616  [swimming, uninspired, not, like, none, joint,...     132  \n",
       "1617  [cast, theatres, came, came, thoughtly, modern...      83  \n",
       "1618  [w, num, segments, well, learn, min, bonus, al...     204  \n",
       "1619  [something, innocence, double, not, better, in...      98  \n",
       "1620  [ends, sexual, longing, wondrous, hingle, onsc...     643  \n",
       "1621  [late, night, well, gets, well, done, shows, s...     103  \n",
       "1622  [pride, like, rather, shown, captain, constant...     491  \n",
       "1623  [dvd, takes, quite, well, dvd, well, recommend...      82  \n",
       "1624  [language, ideas, whole, unconvincing, seems, ...     229  \n",
       "1625  [never, film, wonderful, like, personal, possi...     498  \n",
       "1626  [connected, la, one, musique, early, sound, fu...     589  \n",
       "1627  [f, version, including, right, dolby, transfer...     265  \n",
       "1628  [unsympathetic, yes, glover, companionship, gr...     267  \n",
       "1629  [coraline, well, like, might, going, yet, cree...     533  \n",
       "1630  [see, nobody, acting, horrible, funny, speak, ...     354  \n",
       "1631  [one, well, stupid, whole, num, new, venture, ...     263  \n",
       "1632  [dvd, write, maybe, read, three, fox, dvd, sur...      84  \n",
       "1633  [added, field, trip, birds, color, longer, dis...     119  \n",
       "1634  [poor, not, grab, minutes, barely, understand,...     114  \n",
       "1635  [sequels, must, stimpy, kept, send, social, me...     284  \n",
       "1636  [mean, still, highly, right, still, role, got,...     312  \n",
       "\n",
       "[1637 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df1[sample_df1[\"code\"]==\"dvd\"]\n",
    "#sample_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikaterinikatsarou/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/aikaterinikatsarou/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:62: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "sample_df = sample_df1.copy()\n",
    "\n",
    "# chi square for the important features per product category\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train1 = count_vect.fit_transform(sample_df.nohtml.values)\n",
    "features1 = count_vect.get_feature_names()   \n",
    "    \n",
    "cat_chi2score0 = chi2(X_train1, sample_df.code)[0]\n",
    "cat_chi2score1 = chi2(X_train1, sample_df.code)[1]\n",
    "cat_wscores = zip(features1, cat_chi2score0)\n",
    "cat_wchi2 = sorted(cat_wscores, key=lambda x:x[1])\n",
    "#topchi2 = list(zip(*wchi2[-1000:]))\n",
    "cat_topchi2score= cat_wchi2[-1000:]\n",
    "#cat_chi2score0\n",
    "\n",
    "\n",
    "#chi square for the important features per sentiment class\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(sample_df.nohtml[0:2000].values)\n",
    "features = count_vect.get_feature_names()   \n",
    "    \n",
    "chi2score0 = chi2(X_train, sample_df.label[0:2000])[0]\n",
    "chi2score1 = chi2(X_train, sample_df.label[0:2000])[1]\n",
    "wscores = zip(features, chi2score0)\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1])\n",
    "#topchi2 = list(zip(*wchi2[-1000:]))\n",
    "\n",
    "topchi2score= wchi2[-6000:]\n",
    "#topchi2score\n",
    "\n",
    "\n",
    "# use only the important features\n",
    "import collections\n",
    "\n",
    "d4 = collections.OrderedDict((k, v) for k, v in zip(features1, cat_chi2score1) if v<0.05)\n",
    "#print(d4)\n",
    "list4 = [k for k, v in d4.items()]\n",
    "d5 = collections.OrderedDict((k, v) for k, v in zip(features, chi2score1) if v<0.05)\n",
    "list5 = [k for k, v in d5.items() if k not in d4.items()]\n",
    "list6 = [k for k, v in d5.items()]\n",
    "#d2 = collections.OrderedDict((k, v) for k, v in cat_chi2score)\n",
    "d2 = collections.OrderedDict((k, v) for k, v in cat_topchi2score)\n",
    "list3 = [k for k, v in d2.items()]\n",
    "\n",
    "d = collections.OrderedDict((k, v) for k, v in topchi2score)\n",
    "list1 = [k for k, v in d.items() if k not in d2.items()]\n",
    "   \n",
    "# keep the important features    \n",
    "sample_df[\"tokenized1\"] = sample_df.tokenized   \n",
    "for  index, row in sample_df[0:1637].iterrows():\n",
    "   \n",
    "    row[\"tokenized1\"] =  [word for word in row[\"tokenized1\"] if word in list5]\n",
    "    sample_df[0:1645].set_value(index,'tokenized1',row[\"tokenized1\"])   \n",
    "    \n",
    "for  index2, row2 in sample_df[1637:1737].iterrows():\n",
    "   \n",
    "    row2[\"tokenized1\"] =  [word for word in row2[\"tokenized1\"] if word in list6]\n",
    "    sample_df[1637:1737].set_value(index2,'tokenized1',row2[\"tokenized1\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikaterinikatsarou/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sample_df[\"tokenized2\"] = sample_df.tokenized   \n",
    "\n",
    "noun = []\n",
    "for  index, row in sample_df.iterrows():\n",
    "    noun = [word for word,pos in pos_tag(row[\"tokenized2\"]) if pos.startswith('N')]\n",
    "    #print(noun)\n",
    "    row[\"tokenized2\"] =  [word for word in row[\"tokenized2\"] if word not in noun]\n",
    "    sample_df.set_value(index,'tokenized2',row[\"tokenized2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-be117d2df2ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#ft_model = FastText(\"Downloads/cc.en.300.bin\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastText\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#import fastText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#ft_model = fastText.load_model('Downloads/cc.en.300.bin')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_model'"
     ]
    }
   ],
   "source": [
    "#from pyfasttext import FastText\n",
    "#ft_model = FastText(\"Downloads/cc.en.300.bin\")\n",
    "\n",
    "from fastText import load_model\n",
    "#import fastText \n",
    "#ft_model = fastText.load_model('Downloads/cc.en.300.bin')\n",
    "#X1= sample_df.tokenized2\n",
    "X=sample_df.tokenized1\n",
    "X2=sample_df.tokenized2\n",
    "\n",
    "#from fastText import load_model\n",
    "\n",
    "#ft_model = load_model('Downloads/cc.en.300.bin')\n",
    "#n_features = ft_model.get_dimension()\n",
    "#dict1 ={}\n",
    "\n",
    "\n",
    "def df_to_data(df, X):\n",
    "    \"\"\"\n",
    "    Convert a given dataframe to a dataset of inputs for the NN.\n",
    "    \"\"\"\n",
    "    #x = np.zeros((len(df), 1000, n_features), dtype='float32')\n",
    "\n",
    "    #for i, word in enumerate(sample_df['tokenized'].values):\n",
    "    X=sample_df.tokenized\n",
    "    all_words = set(w for words in X for w in words)\n",
    "    for word in all_words:\n",
    "            nums=ft_model.get_word_vector(word).astype('float32')\n",
    "            dict1[word] = nums\n",
    "            \n",
    "     \n",
    "    return dict1     "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fasttext = df_to_data(sample_df, X)\n",
    "fasttext2 = df_to_data(sample_df, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the pretrained fasttext\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "#from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y= sample_df.label\n",
    "import struct \n",
    "\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, probs=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.probs = probs\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(fasttext))])\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(word2vec=self.word2vec)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "   \n",
    "    \n",
    "  \n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, probs=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.probs = probs\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(fasttext))])\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(word2vec=self.word2vec)        \n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "class MeanEmbeddingVectorizer2(object):\n",
    "    def __init__(self, word2vec, probs=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.probs = probs\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(fasttext2))])\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(word2vec=self.word2vec)\n",
    "    \n",
    "    def fit(self, X2, y):\n",
    "        return self\n",
    "            \n",
    " \n",
    "\n",
    "    def transform(self, X2):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X1\n",
    "        ])\n",
    "    \n",
    "    \n",
    "     \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer2(object):\n",
    "    def __init__(self, word2vec, probs=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.probs = probs\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(fasttext2))])\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(word2vec=self.word2vec)        \n",
    "        \n",
    "    def fit(self, X2,y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X2)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X2):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X2\n",
    "            ])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from mlxtend.preprocessing import DenseTransformer \n",
    "    \n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ColumnSelector(BaseEstimator):\n",
    "    \"\"\"Object for selecting specific columns from a data set.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols : array-like (default: None)\n",
    "        A list specifying the feature indices to be selected. For example,\n",
    "        [1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and\n",
    "        ['A','C','D'] to select the name of feature columns A, C and D.\n",
    "        If None, returns all columns in the array.\n",
    "    drop_axis : bool (default=False)\n",
    "        Drops last axis if True and the only one column is selected. This\n",
    "        is useful, e.g., when the ColumnSelector is used for selecting\n",
    "        only one column and the resulting array should be fed to e.g.,\n",
    "        a scikit-learn column selector. E.g., instead of returning an\n",
    "        array with shape (n_samples, 1), drop_axis=True will return an\n",
    "        aray with shape (n_samples,).\n",
    "    Examples\n",
    "    -----------\n",
    "    For usage examples, please see\n",
    "    http://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cols=None, drop_axis=False):\n",
    "        self.cols = cols\n",
    "        self.drop_axis = drop_axis\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\" Return a slice of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_slice : shape = [n_samples, k_features]\n",
    "            Subset of the feature space where k_features <= n_features\n",
    "        \"\"\"\n",
    "        return self.transform(X=X, y=y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\" Return a slice of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_slice : shape = [n_samples, k_features]\n",
    "            Subset of the feature space where k_features <= n_features\n",
    "        \"\"\"\n",
    "\n",
    "        # We use the loc or iloc accessor if the input is a pandas dataframe\n",
    "        if hasattr(X, 'loc') or hasattr(X, 'iloc'):\n",
    "            if type(self.cols) == tuple:\n",
    "                self.cols = list(self.cols)\n",
    "            types = {type(i) for i in self.cols}\n",
    "            if len(types) > 1:\n",
    "                raise ValueError(\n",
    "                    'Elements in `cols` should be all of the same data type.'\n",
    "                )\n",
    "            if isinstance(self.cols[0], int):\n",
    "                t = X.iloc[:, self.cols].values\n",
    "            elif isinstance(self.cols[0], str):\n",
    "                t = X.loc[:, self.cols].values\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Elements in `cols` should be either `int` or `str`.'\n",
    "                )\n",
    "        else:\n",
    "            t = X[:, self.cols]\n",
    "\n",
    "        if t.shape[-1] == 1 and self.drop_axis:\n",
    "            t = t.reshape(-1)\n",
    "        if len(t.shape) == 1 and not self.drop_axis:\n",
    "            t = t[:, np.newaxis]\n",
    "        return t\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Mock method. Does nothing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        self\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "#import mlxtend\n",
    "#pipe1 = make_pipeline(ColumnSelector(cols=(7,)), MeanEmbeddingVectorizer2(fasttext2), LogisticRegression(\"l1\", random_state=0))\n",
    "#pipe2 = make_pipeline(ColumnSelector(cols=(5, )), TfidfEmbeddingVectorizer(fasttext), SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))\n",
    "\n",
    "#sclf = StackingClassifier(classifiers=[pipe1, pipe2], \n",
    "                         # meta_classifier=LogisticRegression())\n",
    "# Fit ensemble\n",
    "#sclf.fit(sample_df[0:1638], sample_df.label[0:1638].values)\n",
    "\n",
    "# Predict\n",
    "#preds = sclf.predict(sample_df[1639:])\n",
    "\n",
    "#accuracy=accuracy_score(sample_df.label[1639:], preds)\n",
    "#print(accuracy)\n",
    "\n",
    "log_reg_fasttext_tfidf = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer(fasttext)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "\n",
    "log_reg_fasttext2 = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer(fasttext)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "\n",
    "svm_fasttext = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer(fasttext)), \n",
    "                            (\"LinearSVC\", SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))])\n",
    "\n",
    "svm_fasttext_tfidf = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer(fasttext)), \n",
    "                            (\"LinearSVC\", SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))])\n",
    "\n",
    "log_reg_fasttext_tfidf2 = Pipeline([(\"col_sel\", ColumnSelector(cols=8, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer2(fasttext2)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "#pipe_rf = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer2(fasttext2)),\n",
    "#                        ('clf', RandomForestClassifier(n_estimators = 140, max_features = 60, max_depth =120,\n",
    "#                                criterion = \"gini\",min_samples_split = 5, min_samples_leaf= 2,\n",
    "#                                                       random_state=0))])\n",
    "\n",
    "svm_fasttext_tfidf2 = Pipeline([(\"col_sel\", ColumnSelector(cols=8, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer2(fasttext2)), \n",
    "                            (\"LinearSVC\", SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from mlxtend.preprocessing import DenseTransformer \n",
    "    \n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ColumnSelector(BaseEstimator):\n",
    "    \"\"\"Object for selecting specific columns from a data set.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols : array-like (default: None)\n",
    "        A list specifying the feature indices to be selected. For example,\n",
    "        [1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and\n",
    "        ['A','C','D'] to select the name of feature columns A, C and D.\n",
    "        If None, returns all columns in the array.\n",
    "    drop_axis : bool (default=False)\n",
    "        Drops last axis if True and the only one column is selected. This\n",
    "        is useful, e.g., when the ColumnSelector is used for selecting\n",
    "        only one column and the resulting array should be fed to e.g.,\n",
    "        a scikit-learn column selector. E.g., instead of returning an\n",
    "        array with shape (n_samples, 1), drop_axis=True will return an\n",
    "        aray with shape (n_samples,).\n",
    "    Examples\n",
    "    -----------\n",
    "    For usage examples, please see\n",
    "    http://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cols=None, drop_axis=False):\n",
    "        self.cols = cols\n",
    "        self.drop_axis = drop_axis\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\" Return a slice of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_slice : shape = [n_samples, k_features]\n",
    "            Subset of the feature space where k_features <= n_features\n",
    "        \"\"\"\n",
    "        return self.transform(X=X, y=y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\" Return a slice of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_slice : shape = [n_samples, k_features]\n",
    "            Subset of the feature space where k_features <= n_features\n",
    "        \"\"\"\n",
    "\n",
    "        # We use the loc or iloc accessor if the input is a pandas dataframe\n",
    "        if hasattr(X, 'loc') or hasattr(X, 'iloc'):\n",
    "            if type(self.cols) == tuple:\n",
    "                self.cols = list(self.cols)\n",
    "            types = {type(i) for i in self.cols}\n",
    "            if len(types) > 1:\n",
    "                raise ValueError(\n",
    "                    'Elements in `cols` should be all of the same data type.'\n",
    "                )\n",
    "            if isinstance(self.cols[0], int):\n",
    "                t = X.iloc[:, self.cols].values\n",
    "            elif isinstance(self.cols[0], str):\n",
    "                t = X.loc[:, self.cols].values\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Elements in `cols` should be either `int` or `str`.'\n",
    "                )\n",
    "        else:\n",
    "            t = X[:, self.cols]\n",
    "\n",
    "        if t.shape[-1] == 1 and self.drop_axis:\n",
    "            t = t.reshape(-1)\n",
    "        if len(t.shape) == 1 and not self.drop_axis:\n",
    "            t = t[:, np.newaxis]\n",
    "        return t\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Mock method. Does nothing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        self\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "#import mlxtend\n",
    "#pipe1 = make_pipeline(ColumnSelector(cols=(7,)), MeanEmbeddingVectorizer2(fasttext2), LogisticRegression(\"l1\", random_state=0))\n",
    "#pipe2 = make_pipeline(ColumnSelector(cols=(5, )), TfidfEmbeddingVectorizer(fasttext), SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))\n",
    "\n",
    "#sclf = StackingClassifier(classifiers=[pipe1, pipe2], \n",
    "                         # meta_classifier=LogisticRegression())\n",
    "# Fit ensemble\n",
    "#sclf.fit(sample_df[0:1638], sample_df.label[0:1638].values)\n",
    "\n",
    "# Predict\n",
    "#preds = sclf.predict(sample_df[1639:])\n",
    "\n",
    "#accuracy=accuracy_score(sample_df.label[1639:], preds)\n",
    "#print(accuracy)\n",
    "\n",
    "log_reg_fasttext_tfidf = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer(fasttext)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "\n",
    "log_reg_fasttext2 = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer(fasttext)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "\n",
    "svm_fasttext = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer(fasttext)), \n",
    "                            (\"LinearSVC\", SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))])\n",
    "\n",
    "\n",
    "log_reg_fasttext_tfidf2 = Pipeline([(\"col_sel\", ColumnSelector(cols=8, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer2(fasttext2)),\n",
    "                        (\"log_reg\", LogisticRegression(\"l2\", random_state=0))])\n",
    "#pipe_rf = Pipeline([(\"col_sel\", ColumnSelector(cols=7, drop_axis=True)), (\"fasttext vectorizer\", MeanEmbeddingVectorizer2(fasttext2)),\n",
    "#                        ('clf', RandomForestClassifier(n_estimators = 140, max_features = 60, max_depth =120,\n",
    "#                                criterion = \"gini\",min_samples_split = 5, min_samples_leaf= 2,\n",
    "#                                                       random_state=0))])\n",
    "\n",
    "svm_fasttext_tfidf = Pipeline([(\"col_sel\", ColumnSelector(cols=8, drop_axis=True)), (\"fasttext vectorizer\", TfidfEmbeddingVectorizer2(fasttext2)), \n",
    "                            (\"LinearSVC\", SVC(random_state=0, kernel=\"linear\", tol=1e-5, probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "seed = 0\n",
    "#np.random.seed(seed)\n",
    "ensemble = SuperLearner(scorer=metrics.accuracy_score, random_state=seed)\n",
    "\n",
    "# Build the first -rflayer\n",
    "ensemble.add([svm_fasttext, svm_fasttext_tfidf, log_reg_fasttext_tfidf2] )\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(LogisticRegression(\"l2\", random_state=0))\n",
    "# --- Use ---\n",
    "\n",
    "# Fit ensemble\n",
    "ensemble.fit(sample_df[0:1737].values, sample_df.label[0:1737].values)\n",
    "\n",
    "# Predict\n",
    "preds = ensemble.predict(sample_df[1737:].values)\n",
    "\n",
    "accuracy=accuracy_score(sample_df.label[1737:].values, preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>label</th>\n",
       "      <th>#words</th>\n",
       "      <th>code</th>\n",
       "      <th>nohtml</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>#token</th>\n",
       "      <th>tokenized1</th>\n",
       "      <th>tokenized2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i  wet  be_annoying  all  i_think  metal  be_e...</td>\n",
       "      <td>1</td>\n",
       "      <td>198.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>wet annoying think metal expected solid handle...</td>\n",
       "      <td>[wet, annoying, think, metal, expected, solid,...</td>\n",
       "      <td>102</td>\n",
       "      <td>[wet, annoying, think, metal, expected, solid,...</td>\n",
       "      <td>[annoying, think, expected, solid, solid, expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work_also  there.....buy_them  then_i  around_...</td>\n",
       "      <td>0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>work also buy around use garlic not ever get p...</td>\n",
       "      <td>[work, also, buy, around, use, garlic, not, ev...</td>\n",
       "      <td>109</td>\n",
       "      <td>[work, also, buy, around, use, garlic, not, ev...</td>\n",
       "      <td>[also, around, not, ever, get, find, not, must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boiling  other_than  great_this  so_the  when_...</td>\n",
       "      <td>1</td>\n",
       "      <td>380.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>boiling great near boiling heat coffe day boil...</td>\n",
       "      <td>[boiling, great, near, boiling, heat, coffe, d...</td>\n",
       "      <td>186</td>\n",
       "      <td>[boiling, great, near, boiling, heat, coffe, d...</td>\n",
       "      <td>[boiling, great, near, boiling, boiling, hot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accident  well  the_el-cheapo  hives  fashion....</td>\n",
       "      <td>1</td>\n",
       "      <td>272.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>accident well el cheapo hives fashion without ...</td>\n",
       "      <td>[accident, well, el, cheapo, hives, fashion, w...</td>\n",
       "      <td>140</td>\n",
       "      <td>[accident, well, el, cheapo, hives, fashion, w...</td>\n",
       "      <td>[well, el, without, gives, entering, balanced,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are_concerned  and_when  not_sure  really_unde...</td>\n",
       "      <td>1</td>\n",
       "      <td>718.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>concerned not sure really understood love panc...</td>\n",
       "      <td>[concerned, not, sure, really, understood, lov...</td>\n",
       "      <td>369</td>\n",
       "      <td>[concerned, not, sure, really, understood, lov...</td>\n",
       "      <td>[concerned, not, sure, really, understood, ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>well  worst_brand  no_matter  nothing_but  cho...</td>\n",
       "      <td>0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>well worst brand not matter nothing choose bak...</td>\n",
       "      <td>[well, worst, brand, not, matter, nothing, cho...</td>\n",
       "      <td>130</td>\n",
       "      <td>[well, worst, brand, not, matter, nothing, cho...</td>\n",
       "      <td>[well, worst, not, better, non, not, brown, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>save_your  cheaply  i_sent  save  loose_that  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>save cheaply sent save loose right plopped min...</td>\n",
       "      <td>[save, cheaply, sent, save, loose, right, plop...</td>\n",
       "      <td>65</td>\n",
       "      <td>[save, cheaply, sent, save, loose, right, plop...</td>\n",
       "      <td>[save, sent, save, loose, plopped, made, decei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reviews_these  stained_table  size  scratched ...</td>\n",
       "      <td>0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>reviews stained table size scratched use distr...</td>\n",
       "      <td>[reviews, stained, table, size, scratched, use...</td>\n",
       "      <td>166</td>\n",
       "      <td>[reviews, stained, table, size, scratched, use...</td>\n",
       "      <td>[stained, scratched, distructive, behind, make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>description  promised  yesterday--i_just  i_or...</td>\n",
       "      <td>0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>description promised yesterday ordered descrip...</td>\n",
       "      <td>[description, promised, yesterday, ordered, de...</td>\n",
       "      <td>115</td>\n",
       "      <td>[description, promised, yesterday, ordered, de...</td>\n",
       "      <td>[promised, ordered, promised, save, claiming, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>while_it  the_lid  water  without_burning.i'm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>lid water without burning tight tight fill fit...</td>\n",
       "      <td>[lid, water, without, burning, tight, tight, f...</td>\n",
       "      <td>73</td>\n",
       "      <td>[lid, water, without, burning, tight, tight, f...</td>\n",
       "      <td>[lid, without, lid, boiling, mounted, carefull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>it's_got  is_very  works_on  press_is  too  th...</td>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>got works press biggest well not much not skin...</td>\n",
       "      <td>[got, works, press, biggest, well, not, much, ...</td>\n",
       "      <td>66</td>\n",
       "      <td>[got, works, press, biggest, well, not, much, ...</td>\n",
       "      <td>[got, biggest, well, not, much, not, made, beh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i  the_\"head\"  the_last  base  moving_it  it_s...</td>\n",
       "      <td>1</td>\n",
       "      <td>344.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>head last base moving seems day place day way ...</td>\n",
       "      <td>[head, last, base, moving, seems, day, place, ...</td>\n",
       "      <td>164</td>\n",
       "      <td>[head, last, base, moving, seems, day, place, ...</td>\n",
       "      <td>[last, moving, seems, sound, great, slowed, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>always  cookware_solid  bonus  solid  still  a...</td>\n",
       "      <td>0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>always cookware solid bonus solid still availa...</td>\n",
       "      <td>[always, cookware, solid, bonus, solid, still,...</td>\n",
       "      <td>76</td>\n",
       "      <td>[always, cookware, solid, bonus, solid, still,...</td>\n",
       "      <td>[always, solid, solid, still, always, like, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>steel  cookware  recommend_it  keep  stockpot_...</td>\n",
       "      <td>1</td>\n",
       "      <td>210.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>steel cookware recommend keep stockpot lid lid...</td>\n",
       "      <td>[steel, cookware, recommend, keep, stockpot, l...</td>\n",
       "      <td>118</td>\n",
       "      <td>[steel, cookware, recommend, keep, stockpot, l...</td>\n",
       "      <td>[recommend, keep, lid, lid, essential, lid, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>about  &lt;num&gt;_uses...then  failure_so  maximum ...</td>\n",
       "      <td>0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>num uses failure maximum one thickness cut num...</td>\n",
       "      <td>[num, uses, failure, maximum, one, thickness, ...</td>\n",
       "      <td>101</td>\n",
       "      <td>[num, uses, failure, maximum, one, thickness, ...</td>\n",
       "      <td>[maximum, one, buy, opening, different, new, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>recommend_it  sear  whole_pan  product_and  ye...</td>\n",
       "      <td>1</td>\n",
       "      <td>314.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>recommend sear whole pan product years keep pa...</td>\n",
       "      <td>[recommend, sear, whole, pan, product, years, ...</td>\n",
       "      <td>157</td>\n",
       "      <td>[recommend, sear, whole, pan, product, years, ...</td>\n",
       "      <td>[sear, whole, keep, great, thick, owned, nicel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>house.i_used  thing_clean.there  of_heavy  sta...</td>\n",
       "      <td>1</td>\n",
       "      <td>634.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>house used thing clean heavy stain ft one carp...</td>\n",
       "      <td>[house, used, thing, clean, heavy, stain, ft, ...</td>\n",
       "      <td>323</td>\n",
       "      <td>[house, used, thing, clean, heavy, stain, ft, ...</td>\n",
       "      <td>[used, clean, heavy, one, not, large, dropped,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>good_reviews  as_i  cooker  meant  i_was  was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>good reviews cooker meant reviews review times...</td>\n",
       "      <td>[good, reviews, cooker, meant, reviews, review...</td>\n",
       "      <td>86</td>\n",
       "      <td>[good, reviews, cooker, meant, reviews, review...</td>\n",
       "      <td>[good, perfectly, say, decided, really, withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i  time_cleaning  around_for  i_wonder  pan  b...</td>\n",
       "      <td>0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>time cleaning around wonder pan buying analon ...</td>\n",
       "      <td>[time, cleaning, around, wonder, pan, buying, ...</td>\n",
       "      <td>177</td>\n",
       "      <td>[time, cleaning, around, wonder, pan, buying, ...</td>\n",
       "      <td>[cleaning, around, wonder, buying, bought, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bought_&lt;num&gt;  slicer  those_people  pineapple_...</td>\n",
       "      <td>1</td>\n",
       "      <td>124.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>bought num slicer people pineapple easy buying...</td>\n",
       "      <td>[bought, num, slicer, people, pineapple, easy,...</td>\n",
       "      <td>57</td>\n",
       "      <td>[bought, num, slicer, people, pineapple, easy,...</td>\n",
       "      <td>[bought, easy, recieved, also, loved, recieved...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>itself  is_nice  doesn't_feel  any_other  feat...</td>\n",
       "      <td>1</td>\n",
       "      <td>198.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>nice not feel feathers label machine washable ...</td>\n",
       "      <td>[nice, not, feel, feathers, label, machine, wa...</td>\n",
       "      <td>87</td>\n",
       "      <td>[nice, not, feel, feathers, label, machine, wa...</td>\n",
       "      <td>[nice, not, washable, low, nice, well, filled,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sauces  use_it  cycle  lot  discolored  its_ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>sauces use cycle lot discolored new dishwasher...</td>\n",
       "      <td>[sauces, use, cycle, lot, discolored, new, dis...</td>\n",
       "      <td>83</td>\n",
       "      <td>[sauces, use, cycle, lot, discolored, new, dis...</td>\n",
       "      <td>[discolored, new, never, got, got, got, lighte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>made_it  parts_the  seeds_spend  a_second  lar...</td>\n",
       "      <td>0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>made parts seeds spend second large portion se...</td>\n",
       "      <td>[made, parts, seeds, spend, second, large, por...</td>\n",
       "      <td>232</td>\n",
       "      <td>[made, parts, seeds, spend, second, large, por...</td>\n",
       "      <td>[made, spend, second, large, seems, intact, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>the_top  off_fine  used_this  you_open  diffic...</td>\n",
       "      <td>1</td>\n",
       "      <td>188.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>top fine used open difficult pour top otherwis...</td>\n",
       "      <td>[top, fine, used, open, difficult, pour, top, ...</td>\n",
       "      <td>92</td>\n",
       "      <td>[top, fine, used, open, difficult, pour, top, ...</td>\n",
       "      <td>[top, used, open, difficult, pour, top, not, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pretty_much  makes_getting  i_noticed  look  i...</td>\n",
       "      <td>1</td>\n",
       "      <td>308.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>pretty much makes getting noticed look seems w...</td>\n",
       "      <td>[pretty, much, makes, getting, noticed, look, ...</td>\n",
       "      <td>164</td>\n",
       "      <td>[pretty, much, makes, getting, noticed, look, ...</td>\n",
       "      <td>[pretty, much, makes, getting, noticed, seems,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>will_assure  array_of  quality_design  assure ...</td>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>assure array quality design assure handles sty...</td>\n",
       "      <td>[assure, array, quality, design, assure, handl...</td>\n",
       "      <td>75</td>\n",
       "      <td>[assure, array, quality, design, assure, handl...</td>\n",
       "      <td>[stylish, basic, around, long, included, basic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>i  i've  tassimo  to_spend  luck  of_coffee  b...</td>\n",
       "      <td>0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>tassimo spend luck coffee bought pay price cho...</td>\n",
       "      <td>[tassimo, spend, luck, coffee, bought, pay, pr...</td>\n",
       "      <td>280</td>\n",
       "      <td>[tassimo, spend, luck, coffee, bought, pay, pr...</td>\n",
       "      <td>[bought, worse, go, returning, quick, hot, har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>burnt  only_coffee  terrific  but_so  love_thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>216.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>burnt coffee terrific love fact fact get coffe...</td>\n",
       "      <td>[burnt, coffee, terrific, love, fact, fact, ge...</td>\n",
       "      <td>103</td>\n",
       "      <td>[burnt, coffee, terrific, love, fact, fact, ge...</td>\n",
       "      <td>[burnt, love, great, far, works, works, great,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>not_up  typical_cuisinart  standards_while  &lt;n...</td>\n",
       "      <td>0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>not typical cuisinart standards num degree cle...</td>\n",
       "      <td>[not, typical, cuisinart, standards, num, degr...</td>\n",
       "      <td>73</td>\n",
       "      <td>[not, typical, cuisinart, standards, num, degr...</td>\n",
       "      <td>[not, typical, typical, definitely, not, pass,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>i  been_returned  seconds_as  by_another  the_...</td>\n",
       "      <td>0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>returned seconds another clothes gobs beat pre...</td>\n",
       "      <td>[returned, seconds, another, clothes, gobs, be...</td>\n",
       "      <td>193</td>\n",
       "      <td>[returned, seconds, another, clothes, gobs, be...</td>\n",
       "      <td>[returned, another, pretty, assembled, disappo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>w  dropping  sd_memory  to_worry  hx2415  to_b...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>w dropping sd memory worry hx belt hp case goo...</td>\n",
       "      <td>[w, dropping, sd, memory, worry, hx, belt, hp,...</td>\n",
       "      <td>68</td>\n",
       "      <td>[w, dropping, sd, memory, worry, hx, belt, hp,...</td>\n",
       "      <td>[dropping, sd, good, not, tightly, dropping, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>input_using  my_cable  look  slingbox  look_at...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>input using cable look slingbox look flawless ...</td>\n",
       "      <td>[input, using, cable, look, slingbox, look, fl...</td>\n",
       "      <td>122</td>\n",
       "      <td>[input, using, cable, look, slingbox, look, fl...</td>\n",
       "      <td>[using, flawless, first, ever, anywhere, ever,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>pay  then_with  trepidation_i  almost  cost  s...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>pay trepidation almost cost reviewers lower me...</td>\n",
       "      <td>[pay, trepidation, almost, cost, reviewers, lo...</td>\n",
       "      <td>161</td>\n",
       "      <td>[pay, trepidation, almost, cost, reviewers, lo...</td>\n",
       "      <td>[almost, lower, maybe, said, must, even, wall,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>available  series_you  the_obvious  clip  have...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>available series obvious clip varying vs num c...</td>\n",
       "      <td>[available, series, obvious, clip, varying, vs...</td>\n",
       "      <td>236</td>\n",
       "      <td>[available, series, obvious, clip, varying, vs...</td>\n",
       "      <td>[available, obvious, vs, hard, obvious, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>area  networks_in  well  to_use  cord  interfe...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>area networks well use cord interference even ...</td>\n",
       "      <td>[area, networks, well, use, cord, interference...</td>\n",
       "      <td>181</td>\n",
       "      <td>[area, networks, well, use, cord, interference...</td>\n",
       "      <td>[well, even, put, found, though, single, long,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>thoroughly  definitely_recommend  quality_in  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>thoroughly definitely recommend quality recomm...</td>\n",
       "      <td>[thoroughly, definitely, recommend, quality, r...</td>\n",
       "      <td>199</td>\n",
       "      <td>[thoroughly, definitely, recommend, quality, r...</td>\n",
       "      <td>[thoroughly, definitely, ever, rich, deep, inn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>m  minimal  time_is  beware  picture-to-pictur...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>minimal time beware picture picture lag type o...</td>\n",
       "      <td>[minimal, time, beware, picture, picture, lag,...</td>\n",
       "      <td>95</td>\n",
       "      <td>[minimal, time, beware, picture, picture, lag,...</td>\n",
       "      <td>[minimal, beware, older, beware, excellent, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>helpful  called  device_after  owned_the  guid...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>helpful called device owned guide bear crashes...</td>\n",
       "      <td>[helpful, called, device, owned, guide, bear, ...</td>\n",
       "      <td>298</td>\n",
       "      <td>[helpful, called, device, owned, guide, bear, ...</td>\n",
       "      <td>[called, owned, guide, stick, not, provides, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>two_weeks  just_quit  the_sound  i_must  not_w...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>two weeks quit sound must not worth disappoint...</td>\n",
       "      <td>[two, weeks, quit, sound, must, not, worth, di...</td>\n",
       "      <td>124</td>\n",
       "      <td>[two, weeks, quit, sound, must, not, worth, di...</td>\n",
       "      <td>[two, must, not, worth, disappointed, read, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>mdr-ex51_earphones  already_owned  low  mdr-ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>mdr ex earphones already owned low mdr ex disa...</td>\n",
       "      <td>[mdr, ex, earphones, already, owned, low, mdr,...</td>\n",
       "      <td>64</td>\n",
       "      <td>[mdr, ex, earphones, already, owned, low, mdr,...</td>\n",
       "      <td>[already, owned, low, disappointed, owned, sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>somewhere  replacement_parts  unrelated  linke...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>somewhere replacement parts unrelated linked f...</td>\n",
       "      <td>[somewhere, replacement, parts, unrelated, lin...</td>\n",
       "      <td>129</td>\n",
       "      <td>[somewhere, replacement, parts, unrelated, lin...</td>\n",
       "      <td>[somewhere, unrelated, linked, first, linked, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>digital_signal  dvr  great_cable  cables  a_th...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>digital signal dvr great cable cables third ge...</td>\n",
       "      <td>[digital, signal, dvr, great, cable, cables, t...</td>\n",
       "      <td>101</td>\n",
       "      <td>[digital, signal, dvr, great, cable, cables, t...</td>\n",
       "      <td>[great, third, less, not, great, not, spend, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>well  come  &lt;num&gt;_days  a_comcast  gave_me  ve...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>well come num days comcast gave well day less ...</td>\n",
       "      <td>[well, come, num, days, comcast, gave, well, d...</td>\n",
       "      <td>91</td>\n",
       "      <td>[well, come, num, days, comcast, gave, well, d...</td>\n",
       "      <td>[well, come, gave, well, less, concluded, comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>they_advised  and_spent  almost  my  gave_me  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>advised spent almost gave product mediocre vos...</td>\n",
       "      <td>[advised, spent, almost, gave, product, medioc...</td>\n",
       "      <td>189</td>\n",
       "      <td>[advised, spent, almost, gave, product, medioc...</td>\n",
       "      <td>[advised, almost, gave, tried, hopefully, whet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>static_when  me  exclusively_listen  wall  kee...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>static exclusively listen wall keeping switche...</td>\n",
       "      <td>[static, exclusively, listen, wall, keeping, s...</td>\n",
       "      <td>1037</td>\n",
       "      <td>[static, exclusively, listen, wall, keeping, s...</td>\n",
       "      <td>[static, exclusively, listen, keeping, many, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>is_extremely  it_means  it.it_is  errors_ultim...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>extremely means errors ultimately ultimately p...</td>\n",
       "      <td>[extremely, means, errors, ultimately, ultimat...</td>\n",
       "      <td>68</td>\n",
       "      <td>[extremely, means, errors, ultimately, ultimat...</td>\n",
       "      <td>[extremely, ultimately, ultimately, lose, poor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>which  resembles  leads_  made_in  was_returne...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>resembles leads made returned sd advertised se...</td>\n",
       "      <td>[resembles, leads, made, returned, sd, adverti...</td>\n",
       "      <td>447</td>\n",
       "      <td>[resembles, leads, made, returned, sd, adverti...</td>\n",
       "      <td>[leads, made, returned, advertised, seemed, bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>is_laughably  while  laughably  sony_has  that...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>laughably laughably sony happens slight advant...</td>\n",
       "      <td>[laughably, laughably, sony, happens, slight, ...</td>\n",
       "      <td>248</td>\n",
       "      <td>[laughably, laughably, sony, happens, slight, ...</td>\n",
       "      <td>[laughably, laughably, slight, late, yet, goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>flat_doesn't  support_suggested  more_money  s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>flat not support suggested money suggested lin...</td>\n",
       "      <td>[flat, not, support, suggested, money, suggest...</td>\n",
       "      <td>156</td>\n",
       "      <td>[flat, not, support, suggested, money, suggest...</td>\n",
       "      <td>[flat, not, suggested, suggested, fallen, orig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>i  i_can  elsewhere_but  paper  hp  premium  i...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>elsewhere paper hp premium works not cheaper p...</td>\n",
       "      <td>[elsewhere, paper, hp, premium, works, not, ch...</td>\n",
       "      <td>63</td>\n",
       "      <td>[elsewhere, paper, hp, premium, works, not, ch...</td>\n",
       "      <td>[elsewhere, works, not, cheaper, works, well, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>barely  adf  does_not  unusual_documents  the_...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>barely adf not unusual documents glass second ...</td>\n",
       "      <td>[barely, adf, not, unusual, documents, glass, ...</td>\n",
       "      <td>310</td>\n",
       "      <td>[barely, adf, not, unusual, documents, glass, ...</td>\n",
       "      <td>[barely, not, unusual, second, clearing, using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>my_first  downloading  my_new  first  updating...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>first downloading new first updating nero buy ...</td>\n",
       "      <td>[first, downloading, new, first, updating, ner...</td>\n",
       "      <td>96</td>\n",
       "      <td>[first, downloading, new, first, updating, ner...</td>\n",
       "      <td>[first, downloading, new, first, buy, not, bur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>worked_howver  stars_but  so  linksys  while_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>worked howver stars linksys still seems dash n...</td>\n",
       "      <td>[worked, howver, stars, linksys, still, seems,...</td>\n",
       "      <td>299</td>\n",
       "      <td>[worked, howver, stars, linksys, still, seems,...</td>\n",
       "      <td>[worked, still, seems, nice, major, frequently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>not_coming  fm_transmitter  good_transmitter  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>not coming fm transmitter good transmitter get...</td>\n",
       "      <td>[not, coming, fm, transmitter, good, transmitt...</td>\n",
       "      <td>68</td>\n",
       "      <td>[not, coming, fm, transmitter, good, transmitt...</td>\n",
       "      <td>[not, coming, good, listening, coming, never, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>i  going  bose  hobby  matched  think  ago  re...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>going bose hobby matched think ago receiver ye...</td>\n",
       "      <td>[going, bose, hobby, matched, think, ago, rece...</td>\n",
       "      <td>93</td>\n",
       "      <td>[going, bose, hobby, matched, think, ago, rece...</td>\n",
       "      <td>[going, matched, think, ago, ago, fresh, match...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>too_tight  your  they_made  hoping_they  them_...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>tight made hoping break month not not way year...</td>\n",
       "      <td>[tight, made, hoping, break, month, not, not, ...</td>\n",
       "      <td>86</td>\n",
       "      <td>[tight, made, hoping, break, month, not, not, ...</td>\n",
       "      <td>[tight, made, hoping, not, not, old, tight, hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>last  what_i  about  very_derable  amazon_is  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>last derable amazon not know get read charging...</td>\n",
       "      <td>[last, derable, amazon, not, know, get, read, ...</td>\n",
       "      <td>64</td>\n",
       "      <td>[last, derable, amazon, not, know, get, read, ...</td>\n",
       "      <td>[last, derable, not, know, charging, charging,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>a_warning  ripped_all  go  been_satisfied  tim...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>warning ripped go satisfied time match decent ...</td>\n",
       "      <td>[warning, ripped, go, satisfied, time, match, ...</td>\n",
       "      <td>263</td>\n",
       "      <td>[warning, ripped, go, satisfied, time, match, ...</td>\n",
       "      <td>[ripped, go, satisfied, going, though, recomme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>aaa  all  display_has  lower  bucks_more  lcd ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>aaa display lower bucks lcd buy greatest num a...</td>\n",
       "      <td>[aaa, display, lower, bucks, lcd, buy, greates...</td>\n",
       "      <td>78</td>\n",
       "      <td>[aaa, display, lower, bucks, lcd, buy, greates...</td>\n",
       "      <td>[lower, lcd, greatest, not, poor, also, lower,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>always  warning  to_tell  sounds  is_happening...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>electronics</td>\n",
       "      <td>always warning tell sounds happening unit lot ...</td>\n",
       "      <td>[always, warning, tell, sounds, happening, uni...</td>\n",
       "      <td>75</td>\n",
       "      <td>[always, warning, tell, sounds, happening, uni...</td>\n",
       "      <td>[always, happening, like, single, fine, sound,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3274 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  label  #words  \\\n",
       "0     i  wet  be_annoying  all  i_think  metal  be_e...      1   198.0   \n",
       "1     work_also  there.....buy_them  then_i  around_...      0   226.0   \n",
       "2     boiling  other_than  great_this  so_the  when_...      1   380.0   \n",
       "3     accident  well  the_el-cheapo  hives  fashion....      1   272.0   \n",
       "4     are_concerned  and_when  not_sure  really_unde...      1   718.0   \n",
       "5     well  worst_brand  no_matter  nothing_but  cho...      0   234.0   \n",
       "6     save_your  cheaply  i_sent  save  loose_that  ...      0   150.0   \n",
       "7     reviews_these  stained_table  size  scratched ...      0   314.0   \n",
       "8     description  promised  yesterday--i_just  i_or...      0   214.0   \n",
       "9     while_it  the_lid  water  without_burning.i'm ...      0   160.0   \n",
       "10    it's_got  is_very  works_on  press_is  too  th...      1   126.0   \n",
       "11    i  the_\"head\"  the_last  base  moving_it  it_s...      1   344.0   \n",
       "12    always  cookware_solid  bonus  solid  still  a...      0   134.0   \n",
       "13    steel  cookware  recommend_it  keep  stockpot_...      1   210.0   \n",
       "14    about  <num>_uses...then  failure_so  maximum ...      0   176.0   \n",
       "15    recommend_it  sear  whole_pan  product_and  ye...      1   314.0   \n",
       "16    house.i_used  thing_clean.there  of_heavy  sta...      1   634.0   \n",
       "17    good_reviews  as_i  cooker  meant  i_was  was ...      0   214.0   \n",
       "18    i  time_cleaning  around_for  i_wonder  pan  b...      0   352.0   \n",
       "19    bought_<num>  slicer  those_people  pineapple_...      1   124.0   \n",
       "20    itself  is_nice  doesn't_feel  any_other  feat...      1   198.0   \n",
       "21    sauces  use_it  cycle  lot  discolored  its_ne...      0   144.0   \n",
       "22    made_it  parts_the  seeds_spend  a_second  lar...      0   452.0   \n",
       "23    the_top  off_fine  used_this  you_open  diffic...      1   188.0   \n",
       "24    pretty_much  makes_getting  i_noticed  look  i...      1   308.0   \n",
       "25    will_assure  array_of  quality_design  assure ...      1   126.0   \n",
       "26    i  i've  tassimo  to_spend  luck  of_coffee  b...      0   558.0   \n",
       "27    burnt  only_coffee  terrific  but_so  love_thi...      1   216.0   \n",
       "28    not_up  typical_cuisinart  standards_while  <n...      0   128.0   \n",
       "29    i  been_returned  seconds_as  by_another  the_...      0   400.0   \n",
       "...                                                 ...    ...     ...   \n",
       "3244  w  dropping  sd_memory  to_worry  hx2415  to_b...      1     NaN   \n",
       "3245  input_using  my_cable  look  slingbox  look_at...      1     NaN   \n",
       "3246  pay  then_with  trepidation_i  almost  cost  s...      1     NaN   \n",
       "3247  available  series_you  the_obvious  clip  have...      1     NaN   \n",
       "3248  area  networks_in  well  to_use  cord  interfe...      0     NaN   \n",
       "3249  thoroughly  definitely_recommend  quality_in  ...      1     NaN   \n",
       "3250  m  minimal  time_is  beware  picture-to-pictur...      1     NaN   \n",
       "3251  helpful  called  device_after  owned_the  guid...      0     NaN   \n",
       "3252  two_weeks  just_quit  the_sound  i_must  not_w...      0     NaN   \n",
       "3253  mdr-ex51_earphones  already_owned  low  mdr-ex...      0     NaN   \n",
       "3254  somewhere  replacement_parts  unrelated  linke...      1     NaN   \n",
       "3255  digital_signal  dvr  great_cable  cables  a_th...      1     NaN   \n",
       "3256  well  come  <num>_days  a_comcast  gave_me  ve...      0     NaN   \n",
       "3257  they_advised  and_spent  almost  my  gave_me  ...      0     NaN   \n",
       "3258  static_when  me  exclusively_listen  wall  kee...      0     NaN   \n",
       "3259  is_extremely  it_means  it.it_is  errors_ultim...      0     NaN   \n",
       "3260  which  resembles  leads_  made_in  was_returne...      1     NaN   \n",
       "3261  is_laughably  while  laughably  sony_has  that...      0     NaN   \n",
       "3262  flat_doesn't  support_suggested  more_money  s...      0     NaN   \n",
       "3263  i  i_can  elsewhere_but  paper  hp  premium  i...      1     NaN   \n",
       "3264  barely  adf  does_not  unusual_documents  the_...      0     NaN   \n",
       "3265  my_first  downloading  my_new  first  updating...      0     NaN   \n",
       "3266  worked_howver  stars_but  so  linksys  while_s...      0     NaN   \n",
       "3267  not_coming  fm_transmitter  good_transmitter  ...      0     NaN   \n",
       "3268  i  going  bose  hobby  matched  think  ago  re...      1     NaN   \n",
       "3269  too_tight  your  they_made  hoping_they  them_...      0     NaN   \n",
       "3270  last  what_i  about  very_derable  amazon_is  ...      0     NaN   \n",
       "3271  a_warning  ripped_all  go  been_satisfied  tim...      1     NaN   \n",
       "3272  aaa  all  display_has  lower  bucks_more  lcd ...      0     NaN   \n",
       "3273  always  warning  to_tell  sounds  is_happening...      1     NaN   \n",
       "\n",
       "             code                                             nohtml  \\\n",
       "0         kitchen  wet annoying think metal expected solid handle...   \n",
       "1         kitchen  work also buy around use garlic not ever get p...   \n",
       "2         kitchen  boiling great near boiling heat coffe day boil...   \n",
       "3         kitchen  accident well el cheapo hives fashion without ...   \n",
       "4         kitchen  concerned not sure really understood love panc...   \n",
       "5         kitchen  well worst brand not matter nothing choose bak...   \n",
       "6         kitchen  save cheaply sent save loose right plopped min...   \n",
       "7         kitchen  reviews stained table size scratched use distr...   \n",
       "8         kitchen  description promised yesterday ordered descrip...   \n",
       "9         kitchen  lid water without burning tight tight fill fit...   \n",
       "10        kitchen  got works press biggest well not much not skin...   \n",
       "11        kitchen  head last base moving seems day place day way ...   \n",
       "12        kitchen  always cookware solid bonus solid still availa...   \n",
       "13        kitchen  steel cookware recommend keep stockpot lid lid...   \n",
       "14        kitchen  num uses failure maximum one thickness cut num...   \n",
       "15        kitchen  recommend sear whole pan product years keep pa...   \n",
       "16        kitchen  house used thing clean heavy stain ft one carp...   \n",
       "17        kitchen  good reviews cooker meant reviews review times...   \n",
       "18        kitchen  time cleaning around wonder pan buying analon ...   \n",
       "19        kitchen  bought num slicer people pineapple easy buying...   \n",
       "20        kitchen  nice not feel feathers label machine washable ...   \n",
       "21        kitchen  sauces use cycle lot discolored new dishwasher...   \n",
       "22        kitchen  made parts seeds spend second large portion se...   \n",
       "23        kitchen  top fine used open difficult pour top otherwis...   \n",
       "24        kitchen  pretty much makes getting noticed look seems w...   \n",
       "25        kitchen  assure array quality design assure handles sty...   \n",
       "26        kitchen  tassimo spend luck coffee bought pay price cho...   \n",
       "27        kitchen  burnt coffee terrific love fact fact get coffe...   \n",
       "28        kitchen  not typical cuisinart standards num degree cle...   \n",
       "29        kitchen  returned seconds another clothes gobs beat pre...   \n",
       "...           ...                                                ...   \n",
       "3244  electronics  w dropping sd memory worry hx belt hp case goo...   \n",
       "3245  electronics  input using cable look slingbox look flawless ...   \n",
       "3246  electronics  pay trepidation almost cost reviewers lower me...   \n",
       "3247  electronics  available series obvious clip varying vs num c...   \n",
       "3248  electronics  area networks well use cord interference even ...   \n",
       "3249  electronics  thoroughly definitely recommend quality recomm...   \n",
       "3250  electronics  minimal time beware picture picture lag type o...   \n",
       "3251  electronics  helpful called device owned guide bear crashes...   \n",
       "3252  electronics  two weeks quit sound must not worth disappoint...   \n",
       "3253  electronics  mdr ex earphones already owned low mdr ex disa...   \n",
       "3254  electronics  somewhere replacement parts unrelated linked f...   \n",
       "3255  electronics  digital signal dvr great cable cables third ge...   \n",
       "3256  electronics  well come num days comcast gave well day less ...   \n",
       "3257  electronics  advised spent almost gave product mediocre vos...   \n",
       "3258  electronics  static exclusively listen wall keeping switche...   \n",
       "3259  electronics  extremely means errors ultimately ultimately p...   \n",
       "3260  electronics  resembles leads made returned sd advertised se...   \n",
       "3261  electronics  laughably laughably sony happens slight advant...   \n",
       "3262  electronics  flat not support suggested money suggested lin...   \n",
       "3263  electronics  elsewhere paper hp premium works not cheaper p...   \n",
       "3264  electronics  barely adf not unusual documents glass second ...   \n",
       "3265  electronics  first downloading new first updating nero buy ...   \n",
       "3266  electronics  worked howver stars linksys still seems dash n...   \n",
       "3267  electronics  not coming fm transmitter good transmitter get...   \n",
       "3268  electronics  going bose hobby matched think ago receiver ye...   \n",
       "3269  electronics  tight made hoping break month not not way year...   \n",
       "3270  electronics  last derable amazon not know get read charging...   \n",
       "3271  electronics  warning ripped go satisfied time match decent ...   \n",
       "3272  electronics  aaa display lower bucks lcd buy greatest num a...   \n",
       "3273  electronics  always warning tell sounds happening unit lot ...   \n",
       "\n",
       "                                              tokenized  #token  \\\n",
       "0     [wet, annoying, think, metal, expected, solid,...     102   \n",
       "1     [work, also, buy, around, use, garlic, not, ev...     109   \n",
       "2     [boiling, great, near, boiling, heat, coffe, d...     186   \n",
       "3     [accident, well, el, cheapo, hives, fashion, w...     140   \n",
       "4     [concerned, not, sure, really, understood, lov...     369   \n",
       "5     [well, worst, brand, not, matter, nothing, cho...     130   \n",
       "6     [save, cheaply, sent, save, loose, right, plop...      65   \n",
       "7     [reviews, stained, table, size, scratched, use...     166   \n",
       "8     [description, promised, yesterday, ordered, de...     115   \n",
       "9     [lid, water, without, burning, tight, tight, f...      73   \n",
       "10    [got, works, press, biggest, well, not, much, ...      66   \n",
       "11    [head, last, base, moving, seems, day, place, ...     164   \n",
       "12    [always, cookware, solid, bonus, solid, still,...      76   \n",
       "13    [steel, cookware, recommend, keep, stockpot, l...     118   \n",
       "14    [num, uses, failure, maximum, one, thickness, ...     101   \n",
       "15    [recommend, sear, whole, pan, product, years, ...     157   \n",
       "16    [house, used, thing, clean, heavy, stain, ft, ...     323   \n",
       "17    [good, reviews, cooker, meant, reviews, review...      86   \n",
       "18    [time, cleaning, around, wonder, pan, buying, ...     177   \n",
       "19    [bought, num, slicer, people, pineapple, easy,...      57   \n",
       "20    [nice, not, feel, feathers, label, machine, wa...      87   \n",
       "21    [sauces, use, cycle, lot, discolored, new, dis...      83   \n",
       "22    [made, parts, seeds, spend, second, large, por...     232   \n",
       "23    [top, fine, used, open, difficult, pour, top, ...      92   \n",
       "24    [pretty, much, makes, getting, noticed, look, ...     164   \n",
       "25    [assure, array, quality, design, assure, handl...      75   \n",
       "26    [tassimo, spend, luck, coffee, bought, pay, pr...     280   \n",
       "27    [burnt, coffee, terrific, love, fact, fact, ge...     103   \n",
       "28    [not, typical, cuisinart, standards, num, degr...      73   \n",
       "29    [returned, seconds, another, clothes, gobs, be...     193   \n",
       "...                                                 ...     ...   \n",
       "3244  [w, dropping, sd, memory, worry, hx, belt, hp,...      68   \n",
       "3245  [input, using, cable, look, slingbox, look, fl...     122   \n",
       "3246  [pay, trepidation, almost, cost, reviewers, lo...     161   \n",
       "3247  [available, series, obvious, clip, varying, vs...     236   \n",
       "3248  [area, networks, well, use, cord, interference...     181   \n",
       "3249  [thoroughly, definitely, recommend, quality, r...     199   \n",
       "3250  [minimal, time, beware, picture, picture, lag,...      95   \n",
       "3251  [helpful, called, device, owned, guide, bear, ...     298   \n",
       "3252  [two, weeks, quit, sound, must, not, worth, di...     124   \n",
       "3253  [mdr, ex, earphones, already, owned, low, mdr,...      64   \n",
       "3254  [somewhere, replacement, parts, unrelated, lin...     129   \n",
       "3255  [digital, signal, dvr, great, cable, cables, t...     101   \n",
       "3256  [well, come, num, days, comcast, gave, well, d...      91   \n",
       "3257  [advised, spent, almost, gave, product, medioc...     189   \n",
       "3258  [static, exclusively, listen, wall, keeping, s...    1037   \n",
       "3259  [extremely, means, errors, ultimately, ultimat...      68   \n",
       "3260  [resembles, leads, made, returned, sd, adverti...     447   \n",
       "3261  [laughably, laughably, sony, happens, slight, ...     248   \n",
       "3262  [flat, not, support, suggested, money, suggest...     156   \n",
       "3263  [elsewhere, paper, hp, premium, works, not, ch...      63   \n",
       "3264  [barely, adf, not, unusual, documents, glass, ...     310   \n",
       "3265  [first, downloading, new, first, updating, ner...      96   \n",
       "3266  [worked, howver, stars, linksys, still, seems,...     299   \n",
       "3267  [not, coming, fm, transmitter, good, transmitt...      68   \n",
       "3268  [going, bose, hobby, matched, think, ago, rece...      93   \n",
       "3269  [tight, made, hoping, break, month, not, not, ...      86   \n",
       "3270  [last, derable, amazon, not, know, get, read, ...      64   \n",
       "3271  [warning, ripped, go, satisfied, time, match, ...     263   \n",
       "3272  [aaa, display, lower, bucks, lcd, buy, greates...      78   \n",
       "3273  [always, warning, tell, sounds, happening, uni...      75   \n",
       "\n",
       "                                             tokenized1  \\\n",
       "0     [wet, annoying, think, metal, expected, solid,...   \n",
       "1     [work, also, buy, around, use, garlic, not, ev...   \n",
       "2     [boiling, great, near, boiling, heat, coffe, d...   \n",
       "3     [accident, well, el, cheapo, hives, fashion, w...   \n",
       "4     [concerned, not, sure, really, understood, lov...   \n",
       "5     [well, worst, brand, not, matter, nothing, cho...   \n",
       "6     [save, cheaply, sent, save, loose, right, plop...   \n",
       "7     [reviews, stained, table, size, scratched, use...   \n",
       "8     [description, promised, yesterday, ordered, de...   \n",
       "9     [lid, water, without, burning, tight, tight, f...   \n",
       "10    [got, works, press, biggest, well, not, much, ...   \n",
       "11    [head, last, base, moving, seems, day, place, ...   \n",
       "12    [always, cookware, solid, bonus, solid, still,...   \n",
       "13    [steel, cookware, recommend, keep, stockpot, l...   \n",
       "14    [num, uses, failure, maximum, one, thickness, ...   \n",
       "15    [recommend, sear, whole, pan, product, years, ...   \n",
       "16    [house, used, thing, clean, heavy, stain, ft, ...   \n",
       "17    [good, reviews, cooker, meant, reviews, review...   \n",
       "18    [time, cleaning, around, wonder, pan, buying, ...   \n",
       "19    [bought, num, slicer, people, pineapple, easy,...   \n",
       "20    [nice, not, feel, feathers, label, machine, wa...   \n",
       "21    [sauces, use, cycle, lot, discolored, new, dis...   \n",
       "22    [made, parts, seeds, spend, second, large, por...   \n",
       "23    [top, fine, used, open, difficult, pour, top, ...   \n",
       "24    [pretty, much, makes, getting, noticed, look, ...   \n",
       "25    [assure, array, quality, design, assure, handl...   \n",
       "26    [tassimo, spend, luck, coffee, bought, pay, pr...   \n",
       "27    [burnt, coffee, terrific, love, fact, fact, ge...   \n",
       "28    [not, typical, cuisinart, standards, num, degr...   \n",
       "29    [returned, seconds, another, clothes, gobs, be...   \n",
       "...                                                 ...   \n",
       "3244  [w, dropping, sd, memory, worry, hx, belt, hp,...   \n",
       "3245  [input, using, cable, look, slingbox, look, fl...   \n",
       "3246  [pay, trepidation, almost, cost, reviewers, lo...   \n",
       "3247  [available, series, obvious, clip, varying, vs...   \n",
       "3248  [area, networks, well, use, cord, interference...   \n",
       "3249  [thoroughly, definitely, recommend, quality, r...   \n",
       "3250  [minimal, time, beware, picture, picture, lag,...   \n",
       "3251  [helpful, called, device, owned, guide, bear, ...   \n",
       "3252  [two, weeks, quit, sound, must, not, worth, di...   \n",
       "3253  [mdr, ex, earphones, already, owned, low, mdr,...   \n",
       "3254  [somewhere, replacement, parts, unrelated, lin...   \n",
       "3255  [digital, signal, dvr, great, cable, cables, t...   \n",
       "3256  [well, come, num, days, comcast, gave, well, d...   \n",
       "3257  [advised, spent, almost, gave, product, medioc...   \n",
       "3258  [static, exclusively, listen, wall, keeping, s...   \n",
       "3259  [extremely, means, errors, ultimately, ultimat...   \n",
       "3260  [resembles, leads, made, returned, sd, adverti...   \n",
       "3261  [laughably, laughably, sony, happens, slight, ...   \n",
       "3262  [flat, not, support, suggested, money, suggest...   \n",
       "3263  [elsewhere, paper, hp, premium, works, not, ch...   \n",
       "3264  [barely, adf, not, unusual, documents, glass, ...   \n",
       "3265  [first, downloading, new, first, updating, ner...   \n",
       "3266  [worked, howver, stars, linksys, still, seems,...   \n",
       "3267  [not, coming, fm, transmitter, good, transmitt...   \n",
       "3268  [going, bose, hobby, matched, think, ago, rece...   \n",
       "3269  [tight, made, hoping, break, month, not, not, ...   \n",
       "3270  [last, derable, amazon, not, know, get, read, ...   \n",
       "3271  [warning, ripped, go, satisfied, time, match, ...   \n",
       "3272  [aaa, display, lower, bucks, lcd, buy, greates...   \n",
       "3273  [always, warning, tell, sounds, happening, uni...   \n",
       "\n",
       "                                             tokenized2  \n",
       "0     [annoying, think, expected, solid, solid, expe...  \n",
       "1     [also, around, not, ever, get, find, not, must...  \n",
       "2     [boiling, great, near, boiling, boiling, hot, ...  \n",
       "3     [well, el, without, gives, entering, balanced,...  \n",
       "4     [concerned, not, sure, really, understood, ano...  \n",
       "5     [well, worst, not, better, non, not, brown, ho...  \n",
       "6     [save, sent, save, loose, plopped, made, decei...  \n",
       "7     [stained, scratched, distructive, behind, make...  \n",
       "8     [promised, ordered, promised, save, claiming, ...  \n",
       "9     [lid, without, lid, boiling, mounted, carefull...  \n",
       "10    [got, biggest, well, not, much, not, made, beh...  \n",
       "11    [last, moving, seems, sound, great, slowed, ex...  \n",
       "12    [always, solid, solid, still, always, like, so...  \n",
       "13    [recommend, keep, lid, lid, essential, lid, re...  \n",
       "14    [maximum, one, buy, opening, different, new, o...  \n",
       "15    [sear, whole, keep, great, thick, owned, nicel...  \n",
       "16    [used, clean, heavy, one, not, large, dropped,...  \n",
       "17    [good, perfectly, say, decided, really, withou...  \n",
       "18    [cleaning, around, wonder, buying, bought, not...  \n",
       "19    [bought, easy, recieved, also, loved, recieved...  \n",
       "20    [nice, not, washable, low, nice, well, filled,...  \n",
       "21    [discolored, new, never, got, got, got, lighte...  \n",
       "22    [made, spend, second, large, seems, intact, la...  \n",
       "23    [top, used, open, difficult, pour, top, not, g...  \n",
       "24    [pretty, much, makes, getting, noticed, seems,...  \n",
       "25    [stylish, basic, around, long, included, basic...  \n",
       "26    [bought, worse, go, returning, quick, hot, har...  \n",
       "27    [burnt, love, great, far, works, works, great,...  \n",
       "28    [not, typical, typical, definitely, not, pass,...  \n",
       "29    [returned, another, pretty, assembled, disappo...  \n",
       "...                                                 ...  \n",
       "3244  [dropping, sd, good, not, tightly, dropping, s...  \n",
       "3245  [using, flawless, first, ever, anywhere, ever,...  \n",
       "3246  [almost, lower, maybe, said, must, even, wall,...  \n",
       "3247  [available, obvious, vs, hard, obvious, good, ...  \n",
       "3248  [well, even, put, found, though, single, long,...  \n",
       "3249  [thoroughly, definitely, ever, rich, deep, inn...  \n",
       "3250  [minimal, beware, older, beware, excellent, ma...  \n",
       "3251  [called, owned, guide, stick, not, provides, s...  \n",
       "3252  [two, must, not, worth, disappointed, read, fi...  \n",
       "3253  [already, owned, low, disappointed, owned, sou...  \n",
       "3254  [somewhere, unrelated, linked, first, linked, ...  \n",
       "3255  [great, third, less, not, great, not, spend, a...  \n",
       "3256  [well, come, gave, well, less, concluded, comp...  \n",
       "3257  [advised, almost, gave, tried, hopefully, whet...  \n",
       "3258  [static, exclusively, listen, keeping, many, a...  \n",
       "3259  [extremely, ultimately, ultimately, lose, poor...  \n",
       "3260  [leads, made, returned, advertised, seemed, bo...  \n",
       "3261  [laughably, laughably, slight, late, yet, goin...  \n",
       "3262  [flat, not, suggested, suggested, fallen, orig...  \n",
       "3263  [elsewhere, works, not, cheaper, works, well, ...  \n",
       "3264  [barely, not, unusual, second, clearing, using...  \n",
       "3265  [first, downloading, new, first, buy, not, bur...  \n",
       "3266  [worked, still, seems, nice, major, frequently...  \n",
       "3267  [not, coming, good, listening, coming, never, ...  \n",
       "3268  [going, matched, think, ago, ago, fresh, match...  \n",
       "3269  [tight, made, hoping, not, not, old, tight, hu...  \n",
       "3270  [last, derable, not, know, charging, charging,...  \n",
       "3271  [ripped, go, satisfied, going, though, recomme...  \n",
       "3272  [lower, lcd, greatest, not, poor, also, lower,...  \n",
       "3273  [always, happening, like, single, fine, sound,...  \n",
       "\n",
       "[3274 rows x 9 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
