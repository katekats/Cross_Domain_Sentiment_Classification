{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#import json\n",
    "#import optparse\n",
    "import os, regex as re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#import libraries \n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    list1 = []\n",
    "    reviews = []\n",
    "    with open(filename, 'r+') as fr:\n",
    "        print(fr)\n",
    "        end_of_review = 0\n",
    "        for line in fr:\n",
    "            #print(\"is not empty\")\n",
    "            line = re.sub(r'[:][\\d]', \" \", str(line))\n",
    "            if (re.search(\"#label#:negative\", str(line))):\n",
    "                line = re.sub(\"#label#:negative\", \" \" ,str(line))\n",
    "                end_of_review=1\n",
    "            if (re.search(\"#label#:positive\", str(line))):\n",
    "                line = re.sub(\"#label#:positive\", \" \" ,str(line))\n",
    "                end_of_review=1    \n",
    "            str1 = str(line)    \n",
    "                #print(\"end-of review\")\n",
    "            if end_of_review == 1:    \n",
    "                reviews.append(str1)\n",
    "                end_of_review = 0\n",
    "              \n",
    "           \n",
    "        return (reviews)\n",
    "            #print(list1)\n",
    "        \n",
    "def convert_to_dataframe(listname):\n",
    "    df1 = pd.DataFrame({'reviews':listname})\n",
    "    return df1\n",
    "\n",
    "\n",
    "def get_label_from_filename(filename, df):\n",
    "    if re.search(\"positive\", str(filename)):\n",
    "        df[\"label\"] = 1\n",
    "    if  re.search(\"negative\", str(filename)):\n",
    "        df[\"label\"] = 0 \n",
    "        #pd.set_option('display.max_colwidth', -1)\n",
    "    return df   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# books_dataset\n",
    "neg_reviews_list = read_data('Downloads/processed_acl/books/negative.review')\n",
    "df1 = convert_to_dataframe(neg_reviews_list)\n",
    "df1 =get_label_from_filename('Downloads/processed_acl/books/negative.review', df1)\n",
    "\n",
    "\n",
    "pos_reviews_list = read_data('Downloads/processed_acl/books/positive.review')\n",
    "df2 = convert_to_dataframe(pos_reviews_list)\n",
    "df2 =get_label_from_filename('Downloads/processed_acl/books/positive.review', df2)\n",
    "\n",
    "\n",
    "df_books = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# dvd_dataset\n",
    "neg_reviews_list2 = read_data('Downloads/processed_acl/dvd/negative.review')\n",
    "df3 = convert_to_dataframe(neg_reviews_list2)\n",
    "df3 =get_label_from_filename('Downloads/processed_acl/dvd/negative.review', df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pos_reviews_list2 = read_data('Downloads/processed_acl/dvd/positive.review')\n",
    "df4 = convert_to_dataframe(pos_reviews_list2)\n",
    "df4 =get_label_from_filename('Downloads/processed_acl/dvd/positive.review', df4)\n",
    "\n",
    "df_dvd = pd.concat([df3, df4], axis=0)\n",
    "\n",
    "# kitchen_dataset\n",
    "neg_reviews_list3 = read_data('Downloads/processed_acl/kitchen/negative.review')\n",
    "df5 = convert_to_dataframe(neg_reviews_list3)\n",
    "df5 =get_label_from_filename('Downloads/processed_acl/kitchen/negative.review', df5)\n",
    "\n",
    "\n",
    "pos_reviews_list3 = read_data('Downloads/processed_acl/kitchen/positive.review')\n",
    "df6 = convert_to_dataframe(pos_reviews_list3)\n",
    "df6 =get_label_from_filename('Downloads/processed_acl/kitchen/positive.review', df6)\n",
    "\n",
    "df_kitchen = pd.concat([df5, df6], axis=0)\n",
    "\n",
    "# electronics_dataset\n",
    "neg_reviews_list4 = read_data('Downloads/processed_acl/electronics/negative.review')\n",
    "df7 = convert_to_dataframe(neg_reviews_list4)\n",
    "df7 =get_label_from_filename('Downloads/processed_acl/electronics/negative.review', df7)\n",
    "\n",
    "\n",
    "pos_reviews_list4 = read_data('Downloads/processed_acl/electronics/positive.review')\n",
    "df8 = convert_to_dataframe(pos_reviews_list4)\n",
    "df8 =get_label_from_filename('Downloads/processed_acl/electronics/positive.review', df8)\n",
    "\n",
    "df_electronics = pd.concat([df7, df8], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#adding column for number of words in review in original data frame\n",
    "df_books['#words'] = df_books.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "df_dvd['#words'] = df_dvd.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "#e_df['#words'] = e_df.reviewText.apply(lambda x: len(str(x).split(' ')))\n",
    "#k_df['#words'] = k_df.reviewText.apply(lambda x: len(str(x).split(' ')))\n",
    "df_kitchen['#words'] = df_kitchen.reviews.apply(lambda x: len(str(x).split(' ')))\n",
    "df_electronics['#words'] = df_electronics.apply(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffling the rows in all the datasets to make them randomly ordered\n",
    "df_books.sample(frac=1)\n",
    "df_books = df_books.sample(frac=1).reset_index(drop=True)\n",
    "df_books[\"code\"] = \"books\"\n",
    "\n",
    "df_dvd.sample(frac=1)\n",
    "df_dvd = df_dvd.sample(frac=1).reset_index(drop=True)\n",
    "df_dvd[\"code\"] = \"dvd\"\n",
    "\n",
    "df_kitchen.sample(frac=1)\n",
    "df_kitchen = df_kitchen.sample(frac=1).reset_index(drop=True)\n",
    "df_kitchen[\"code\"] = \"kitchen\"\n",
    "\n",
    "df_electronics.sample(frac=1)\n",
    "df_electronics = df_electronics.sample(frac=1).reset_index(drop=True)\n",
    "df_electronics[\"code\"] = \"electronics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions for preprocessing steps\n",
    "stop = set(('i','im','ive', 'me','my','myself','we','our','ours','ourselves','you','youre','youve','youll','youd','your','yours','yourself','yourselves','he','him','his','himself','she','shes','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','thatll','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','only','own','same','so','than','too','very','s','t','can','will','just','should','shouldve','now','d','ll','m','o','re','ve','y','ma'))\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def replace_url(df,col,rm1,rm2):\n",
    "    return(df[col].str.replace(rm1,rm2))\n",
    "\n",
    "def extract_emo(df, col, emo):\n",
    "    return(df[col].str.extractall(emo).unstack().apply(lambda x:' '.join(x.dropna()), axis=1))\n",
    "\n",
    "def replace_emo(df,col,emo1,emo2):\n",
    "    return(df[col].str.replace(emo1,emo2))\n",
    "\n",
    "def replace_punct(df, col, punct1, punct2):\n",
    "    return(df[col].str.replace(punct1, punct2))\n",
    "\n",
    "def lower_words(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join(x.lower() for x in x.split())))\n",
    "\n",
    "def remove_stop(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)))\n",
    "\n",
    "def tokenize(df,col):\n",
    "    return(df.apply(lambda row: nltk.word_tokenize(row[col]), axis=1))\n",
    "\n",
    "def word_count(df,col):\n",
    "    return(df[col].apply(lambda x: len(str(x).split(' '))))\n",
    "\n",
    "def stemming(df,col):\n",
    "    return(df[col].apply(lambda x: \" \".join([sno.stem(word) for word in x.split()])))\n",
    "\n",
    "\n",
    "#Step1 Pre-Processing\n",
    "sample_df['nohtml'] = replace_url(sample_df,'reviews','^http?:\\/\\/.*[\\r\\n]*','')\n",
    "#sample_df['emoticons']=extract_emo(sample_df,'nohtml','(=\\)|:\\)|:-\\)|:\\(|:-\\(|;\\)|;-\\)|:D|=D|:-D|=\\(|:P|=P|:-P)')\n",
    "#sample_df['nohtml'] = replace_emo(sample_df,'nohtml','(=\\)|:\\)|:-\\)|:D|=D|:-D)',' happy ')\n",
    "#sample_df['nohtml'] = replace_emo(sample_df,'nohtml','(=\\(|:\\(|:-\\()',' sad ')\n",
    "#sample_df['nohtml'] = replace_emo(sample_df,'nohtml','(;\\)|;-\\))',' wink ')\n",
    "#sample_df['nohtml'] = replace_emo(sample_df,'nohtml','(:P|=P|:-P)',' playful ')\n",
    "sample_df['nohtml'] = lower_words(sample_df,'nohtml')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml', '[^\\w\\s]',' ')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml', '_',' ')\n",
    "sample_df['nohtml'] = replace_punct(sample_df, 'nohtml',r'\\b(no|not|nt|dont|doesnt|didnt|cant|cannt|cannot|wouldnt|wont|couldnt|hasnt|havent|hadnt|shouldnt)\\s+([a-z])',r'not \\2')\n",
    "sample_df['nohtml'] = remove_stop(sample_df,'nohtml')\n",
    "#sample_df['nohtml'] = stemming(sample_df,'nohtml')\n",
    "sample_df['tokenized'] = tokenize(sample_df,'nohtml')\n",
    "sample_df['#token'] = word_count(sample_df,'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_num_nouns(df, col1, col2, col3):\n",
    "    sample_df[col1]=df.tokenized.apply(lambda x : pos_tag(x))\n",
    "    for  index, row in df.iterrows():\n",
    "        row[col1]= [word for word,pos in row[col1] if pos.startswith('N')]\n",
    "    #sample_df.set_value(index,'nouns',row[\"nouns\"])   \n",
    "        sample_df.set_value(index,col1,row[col1])\n",
    "    sample_df[col2]=sample_df[col1].apply(lambda x: len(x)) \n",
    "    return (sample_df[col2].groupby(sample_df[col3]).sum())\n",
    "\n",
    "# find the number of verbs per class\n",
    "def find_num_verbs(df, col1, col2, col3):\n",
    "    sample_df[col1]=df.tokenized.apply(lambda x : pos_tag(x))\n",
    "    for  index, row in df.iterrows():\n",
    "        row[col1]= [word for word,pos in row[col1] if pos.startswith('VB')]\n",
    "    #sample_df.set_value(index,'nouns',row[\"nouns\"])   \n",
    "        sample_df.set_value(index,col1,row[col1])\n",
    "    sample_df[col2]=sample_df[col1].apply(lambda x: len(x)) \n",
    "    return (sample_df[col2].groupby(sample_df[col3]).sum())\n",
    "\n",
    "# find the number of adjectives per class\n",
    "def find_num_adjective(df, col1, col2, col3):\n",
    "    sample_df[col1]=df.tokenized.apply(lambda x : pos_tag(x))\n",
    "    for  index, row in df.iterrows():\n",
    "        row[col1]= [word for word,pos in row[col1] if pos.startswith('JJ')]\n",
    "    #sample_df.set_value(index,'nouns',row[\"nouns\"])   \n",
    "        sample_df.set_value(index,col1,row[col1])\n",
    "    sample_df[col2]=sample_df[col1].apply(lambda x: len(x)) \n",
    "    return (sample_df[col2].groupby(sample_df[col3]).sum())\n",
    "\n",
    "# find the number of adverbs per class\n",
    "def find_num_adverbs(df, col1, col2, col3):\n",
    "    sample_df[col1]=df.tokenized.apply(lambda x : pos_tag(x))\n",
    "    for  index, row in df.iterrows():\n",
    "        row[col1]= [word for word,pos in row[col1] if pos.startswith('RB')]\n",
    "    #sample_df.set_value(index,'nouns',row[\"nouns\"])   \n",
    "        sample_df.set_value(index,col1,row[col1])\n",
    "    sample_df[col2]=sample_df[col1].apply(lambda x: len(x)) \n",
    "    return (sample_df[col2].groupby(sample_df[col3]).sum())\n",
    "\n",
    "# find the number of total words per class expect the spected category\n",
    "def find_num_others(df, col1, col2, col3):\n",
    "    sample_df[col1]=df.tokenized.apply(lambda x : pos_tag(x))\n",
    "    for  index, row in df.iterrows():\n",
    "        row[col1]= [word for word,pos in row[col1]if not pos.startswith('N')]\n",
    "    #sample_df.set_value(index,'nouns',row[\"nouns\"])   \n",
    "        sample_df.set_value(index,col1,row[col1])\n",
    "    sample_df[col2]=sample_df[col1].apply(lambda x: len(x)) \n",
    "    return (sample_df[col2].groupby(sample_df[col3]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_num_verbs(sample_df, \"pos_tagged2\", \"#verbs\", \"label\")\n",
    "find_num_adjective(sample_df, \"pos_tagged3\", \"#adjectives\", \"label\")\n",
    "find_num_adverbs(sample_df, \"pos_tagged4\", \"#adverbs\", \"label\")\n",
    "find_num_others(sample_df, \"pos_tagged5\", \"#words\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### 25\n",
    "# chi-squared test with similar proportions\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "# contingency table\n",
    "table = [[107099, 107515],\n",
    "[151608, 142169]]\n",
    "print(table)\n",
    "stat, p, dof, expected = chi2_contingency(table)\n",
    "print('dof=%d' % dof)\n",
    "print(expected)\n",
    "# interpret test-statistic\n",
    "prob = 0.95\n",
    "critical = chi2.ppf(prob, dof)\n",
    "print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\n",
    "if abs(stat) >= critical:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')\n",
    "# interpret p-value\n",
    "alpha = 1.0 - prob\n",
    "print('significance=%.3f, p=%.3f' % (alpha, p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
